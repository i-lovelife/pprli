{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from src.data.dataset import FergZeroOne, Ferg\n",
    "from src.trainer import PrivaterTrainer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "x:(47401, 64, 64, 3)\n",
      "y:(47401, 7)\n",
      "p:(47401, 6)\n",
      "test data\n",
      "x:(8365, 64, 64, 3)\n",
      "y:(8365, 7)\n",
      "p:(8365, 6)\n"
     ]
    }
   ],
   "source": [
    "ferg = Ferg.from_hdf5(select_people=[0, 1, 2, 3, 4, 5], transform=True)\n",
    "ferg.show_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: g_log=[-1.9381552, 0.39579934, 1.9421132] d_log=2.3640737533569336\n",
      "iter 10: g_log=[-9.786331, 0.12071291, 9.787539] d_log=10.718585968017578\n",
      "iter 20: g_log=[-7.1704655, 0.07216653, 7.171187] d_log=8.22438907623291\n",
      "iter 30: g_log=[-9.288389, 0.06197627, 9.289009] d_log=11.853117942810059\n",
      "iter 40: g_log=[-7.302063, 0.052775875, 7.302591] d_log=9.259737968444824\n",
      "iter 50: g_log=[-9.180052, 0.048414305, 9.180536] d_log=9.950139999389648\n",
      "iter 60: g_log=[-7.9778123, 0.04724876, 7.978285] d_log=8.083293914794922\n",
      "iter 70: g_log=[-6.4088464, 0.046517663, 6.409312] d_log=7.0166168212890625\n",
      "iter 80: g_log=[-6.2590528, 0.047251873, 6.2595253] d_log=6.856193542480469\n",
      "iter 90: g_log=[-4.8286695, 0.047270227, 4.829142] d_log=5.061199188232422\n",
      "iter 100: g_log=[-5.771205, 0.046817794, 5.771673] d_log=6.804755210876465\n",
      "iter 110: g_log=[-3.233931, 0.042256378, 3.2343535] d_log=3.422725200653076\n",
      "iter 120: g_log=[-4.0800977, 0.043036245, 4.0805283] d_log=4.550319671630859\n",
      "iter 130: g_log=[-5.504047, 0.044126417, 5.504488] d_log=6.074283123016357\n",
      "iter 140: g_log=[-3.0600462, 0.041909166, 3.0604653] d_log=3.1149563789367676\n",
      "iter 150: g_log=[-3.3580916, 0.045178186, 3.3585434] d_log=4.483697891235352\n",
      "iter 160: g_log=[-2.5798202, 0.041528795, 2.5802355] d_log=3.0709590911865234\n",
      "iter 170: g_log=[-2.8914464, 0.042892225, 2.8918753] d_log=3.036571502685547\n",
      "iter 180: g_log=[-2.1656425, 0.04266809, 2.1660693] d_log=2.3377525806427\n",
      "iter 190: g_log=[-1.5779209, 0.039360993, 1.5783145] d_log=1.4664089679718018\n",
      "iter 200: g_log=[-1.5289422, 0.038028516, 1.5293225] d_log=1.602016568183899\n",
      "iter 210: g_log=[-3.6021705, 0.039051026, 3.602561] d_log=3.4278488159179688\n",
      "iter 220: g_log=[-2.7165084, 0.041108117, 2.7169194] d_log=2.916900873184204\n",
      "iter 230: g_log=[-1.6514343, 0.038114876, 1.6518154] d_log=1.9393863677978516\n",
      "iter 240: g_log=[-2.1440184, 0.037720107, 2.1443956] d_log=1.9435243606567383\n",
      "iter 250: g_log=[-3.0946462, 0.03569013, 3.0950031] d_log=3.4922897815704346\n",
      "iter 260: g_log=[-2.2241058, 0.036475822, 2.2244706] d_log=2.4848523139953613\n",
      "iter 270: g_log=[-2.062598, 0.03718107, 2.0629697] d_log=1.6763150691986084\n",
      "iter 280: g_log=[-1.851188, 0.035624847, 1.8515441] d_log=2.062955856323242\n",
      "iter 290: g_log=[-1.5682967, 0.035696678, 1.5686536] d_log=1.4871442317962646\n",
      "iter 300: g_log=[-1.7185067, 0.034109026, 1.7188478] d_log=1.7248879671096802\n",
      "iter 310: g_log=[-2.1191504, 0.03448126, 2.1194952] d_log=2.1961607933044434\n",
      "iter 320: g_log=[-2.3081903, 0.034552842, 2.3085358] d_log=2.218409538269043\n",
      "iter 330: g_log=[-1.9517037, 0.031237947, 1.952016] d_log=2.219637870788574\n",
      "iter 340: g_log=[-1.718852, 0.031380594, 1.7191658] d_log=1.7752196788787842\n",
      "iter 350: g_log=[-2.1111386, 0.03137566, 2.1114523] d_log=1.9283876419067383\n",
      "iter 360: g_log=[-2.2072086, 0.030746415, 2.2075162] d_log=2.172421455383301\n",
      "iter 370: g_log=[-2.1230438, 0.033493325, 2.1233788] d_log=2.1300950050354004\n",
      "iter 380: g_log=[-1.704647, 0.031677097, 1.7049637] d_log=1.7711491584777832\n",
      "iter 390: g_log=[-1.652994, 0.029981233, 1.6532938] d_log=1.6706175804138184\n",
      "iter 400: g_log=[-1.6558465, 0.029550038, 1.656142] d_log=1.784137487411499\n",
      "iter 410: g_log=[-2.012358, 0.033267975, 2.0126905] d_log=1.8910412788391113\n",
      "iter 420: g_log=[-1.851525, 0.030815205, 1.8518331] d_log=2.0272960662841797\n",
      "iter 430: g_log=[-1.9560715, 0.03076748, 1.9563792] d_log=2.0246028900146484\n",
      "iter 440: g_log=[-2.0026104, 0.030867744, 2.0029192] d_log=2.1099987030029297\n",
      "iter 450: g_log=[-1.8778168, 0.03142806, 1.878131] d_log=1.9105825424194336\n",
      "iter 460: g_log=[-1.8007156, 0.029828418, 1.8010138] d_log=1.762500286102295\n",
      "iter 470: g_log=[-1.8544519, 0.02834463, 1.8547354] d_log=1.7664636373519897\n",
      "iter 480: g_log=[-2.0325327, 0.029377557, 2.0328264] d_log=2.204820156097412\n",
      "iter 490: g_log=[-1.8523083, 0.027699672, 1.8525853] d_log=2.077183246612549\n",
      "iter 500: g_log=[-1.6567372, 0.027623441, 1.6570134] d_log=1.650374174118042\n",
      "iter 510: g_log=[-2.053598, 0.027734011, 2.0538752] d_log=2.0901072025299072\n",
      "iter 520: g_log=[-2.450089, 0.026366206, 2.4503527] d_log=2.476895332336426\n",
      "iter 530: g_log=[-1.8831348, 0.024772082, 1.8833826] d_log=1.8847107887268066\n",
      "iter 540: g_log=[-1.5408988, 0.024168327, 1.5411404] d_log=1.5731585025787354\n",
      "iter 550: g_log=[-1.5769547, 0.02497622, 1.5772045] d_log=1.6046253442764282\n",
      "iter 560: g_log=[-2.0352812, 0.025337923, 2.0355346] d_log=2.050947427749634\n",
      "iter 570: g_log=[-2.1211786, 0.025377106, 2.1214323] d_log=2.0921335220336914\n",
      "iter 580: g_log=[-1.88331, 0.025633711, 1.8835663] d_log=1.9173167943954468\n",
      "iter 590: g_log=[-1.7129056, 0.024601363, 1.7131517] d_log=1.751474142074585\n",
      "iter 600: g_log=[-1.5730772, 0.023929853, 1.5733165] d_log=1.6367337703704834\n",
      "iter 610: g_log=[-1.6873003, 0.022347474, 1.6875238] d_log=1.6416923999786377\n",
      "iter 620: g_log=[-1.8871268, 0.025279269, 1.8873796] d_log=1.9058576822280884\n",
      "iter 630: g_log=[-1.9184228, 0.024042089, 1.9186633] d_log=1.9461469650268555\n",
      "iter 640: g_log=[-1.9023964, 0.023933616, 1.9026358] d_log=1.8595317602157593\n",
      "iter 650: g_log=[-1.7422104, 0.022418136, 1.7424346] d_log=1.8133714199066162\n",
      "iter 660: g_log=[-1.6978301, 0.024049241, 1.6980705] d_log=1.5950871706008911\n",
      "iter 670: g_log=[-1.7656881, 0.022611983, 1.7659142] d_log=1.768380880355835\n",
      "iter 680: g_log=[-1.8730515, 0.023306027, 1.8732846] d_log=1.8753280639648438\n",
      "iter 690: g_log=[-2.0008576, 0.02468099, 2.0011044] d_log=1.9756379127502441\n",
      "iter 700: g_log=[-1.8519937, 0.021362197, 1.8522073] d_log=1.86515212059021\n",
      "iter 710: g_log=[-1.7271073, 0.021128863, 1.7273185] d_log=1.8278121948242188\n",
      "iter 720: g_log=[-1.6565791, 0.020199127, 1.6567811] d_log=1.6517736911773682\n",
      "iter 730: g_log=[-1.7333695, 0.02043383, 1.7335738] d_log=1.7298834323883057\n",
      "iter 740: g_log=[-1.7465972, 0.019837877, 1.7467955] d_log=1.7595977783203125\n",
      "iter 750: g_log=[-1.7528551, 0.022479273, 1.7530799] d_log=1.8520832061767578\n",
      "iter 760: g_log=[-1.8354461, 0.019457497, 1.8356407] d_log=1.8462772369384766\n",
      "iter 770: g_log=[-1.8258487, 0.018832522, 1.826037] d_log=1.8317077159881592\n",
      "iter 780: g_log=[-1.7866571, 0.02081241, 1.7868652] d_log=1.7825857400894165\n",
      "iter 790: g_log=[-1.7440186, 0.01934441, 1.744212] d_log=1.7578213214874268\n",
      "iter 800: g_log=[-1.767305, 0.018755758, 1.7674925] d_log=1.7948789596557617\n",
      "iter 810: g_log=[-1.8187975, 0.018359799, 1.818981] d_log=1.8287043571472168\n",
      "iter 820: g_log=[-1.8264338, 0.019610697, 1.8266299] d_log=1.8277368545532227\n",
      "iter 830: g_log=[-1.8327006, 0.019564217, 1.8328962] d_log=1.8240176439285278\n",
      "iter 840: g_log=[-1.8619542, 0.019400392, 1.8621482] d_log=1.8336765766143799\n",
      "iter 850: g_log=[-1.743932, 0.02112509, 1.7441432] d_log=1.8078598976135254\n",
      "iter 860: g_log=[-1.7802954, 0.01869224, 1.7804823] d_log=1.787510633468628\n",
      "iter 870: g_log=[-1.790755, 0.019193139, 1.790947] d_log=1.758119821548462\n",
      "iter 880: g_log=[-1.7639172, 0.018880809, 1.764106] d_log=1.7350032329559326\n",
      "iter 890: g_log=[-1.8451928, 0.01877303, 1.8453805] d_log=1.8624614477157593\n",
      "iter 900: g_log=[-1.9930575, 0.021065628, 1.9932681] d_log=1.9677891731262207\n",
      "iter 910: g_log=[-1.8830816, 0.021039778, 1.883292] d_log=1.89652681350708\n",
      "iter 920: g_log=[-1.5976173, 0.019138724, 1.5978086] d_log=1.5269346237182617\n",
      "iter 930: g_log=[-1.687243, 0.021433938, 1.6874573] d_log=1.7234169244766235\n",
      "iter 940: g_log=[-1.8526049, 0.02171889, 1.8528221] d_log=1.8584513664245605\n",
      "iter 950: g_log=[-1.9263974, 0.018335368, 1.9265808] d_log=1.8961548805236816\n",
      "iter 960: g_log=[-1.8107051, 0.017556565, 1.8108807] d_log=1.8105069398880005\n",
      "iter 970: g_log=[-1.7349384, 0.017549507, 1.7351139] d_log=1.7392550706863403\n",
      "iter 980: g_log=[-1.6953666, 0.016474895, 1.6955314] d_log=1.6716231107711792\n",
      "iter 990: g_log=[-1.6864415, 0.016487617, 1.6866064] d_log=1.6827651262283325\n",
      "iter 1000: g_log=[-1.7497233, 0.017161641, 1.749895] d_log=1.7662038803100586\n",
      "iter 1010: g_log=[-1.8578123, 0.017016359, 1.8579824] d_log=1.8379912376403809\n",
      "iter 1020: g_log=[-1.894636, 0.01682263, 1.8948042] d_log=1.8715038299560547\n",
      "iter 1030: g_log=[-1.7330189, 0.016411528, 1.733183] d_log=1.6644394397735596\n",
      "iter 1040: g_log=[-1.8534771, 0.021416992, 1.8536913] d_log=1.9076875448226929\n",
      "iter 1050: g_log=[-2.0096223, 0.016921949, 2.0097916] d_log=1.973207712173462\n",
      "iter 1060: g_log=[-1.8243842, 0.015814418, 1.8245424] d_log=1.784095048904419\n",
      "iter 1070: g_log=[-1.6548052, 0.01555734, 1.6549608] d_log=1.6224397420883179\n",
      "iter 1080: g_log=[-1.6324488, 0.01642153, 1.6326131] d_log=1.6065313816070557\n",
      "iter 1090: g_log=[-1.7336489, 0.016129384, 1.7338102] d_log=1.7428648471832275\n",
      "iter 1100: g_log=[-1.8594679, 0.015440687, 1.8596222] d_log=1.9215638637542725\n",
      "iter 1110: g_log=[-1.9347162, 0.017311908, 1.9348893] d_log=1.9198781251907349\n",
      "iter 1120: g_log=[-1.8679073, 0.016495919, 1.8680723] d_log=1.8517476320266724\n",
      "iter 1130: g_log=[-1.7983838, 0.015720416, 1.7985411] d_log=1.797935128211975\n",
      "iter 1140: g_log=[-1.8045315, 0.016516974, 1.8046967] d_log=1.7991280555725098\n",
      "iter 1150: g_log=[-1.8226385, 0.018257359, 1.8228211] d_log=1.8077068328857422\n",
      "iter 1160: g_log=[-1.739053, 0.014974479, 1.7392027] d_log=1.7619690895080566\n",
      "iter 1170: g_log=[-1.7804853, 0.013602675, 1.7806213] d_log=1.7542545795440674\n",
      "iter 1180: g_log=[-1.7262481, 0.011550813, 1.7263637] d_log=1.785290002822876\n",
      "iter 1190: g_log=[-1.8107865, 0.010870967, 1.8108952] d_log=1.7784197330474854\n",
      "iter 1200: g_log=[-1.891492, 0.015741607, 1.8916495] d_log=1.92277193069458\n",
      "iter 1210: g_log=[-1.8746768, 0.0105044, 1.8747818] d_log=1.8559335470199585\n",
      "iter 1220: g_log=[-1.8210871, 0.011540957, 1.8212025] d_log=1.7897210121154785\n",
      "iter 1230: g_log=[-1.7846917, 0.009820802, 1.7847899] d_log=1.8127270936965942\n",
      "iter 1240: g_log=[-1.7661386, 0.009438392, 1.766233] d_log=1.7449920177459717\n",
      "iter 1250: g_log=[-1.7885278, 0.009587602, 1.7886237] d_log=1.7485108375549316\n",
      "iter 1260: g_log=[-1.7867185, 0.008475369, 1.7868032] d_log=1.7578861713409424\n",
      "iter 1270: g_log=[-1.7682034, 0.008434672, 1.7682878] d_log=1.7873897552490234\n",
      "iter 1280: g_log=[-1.8161811, 0.008692525, 1.816268] d_log=1.7809383869171143\n",
      "iter 1290: g_log=[-1.8089366, 0.0066460976, 1.8090031] d_log=1.791186809539795\n",
      "iter 1300: g_log=[-1.7550462, 0.0081646005, 1.7551279] d_log=1.765453815460205\n",
      "iter 1310: g_log=[-1.7886792, 0.005692129, 1.7887361] d_log=1.7766335010528564\n",
      "iter 1320: g_log=[-1.8350003, 0.0054868814, 1.8350551] d_log=1.7179874181747437\n",
      "iter 1330: g_log=[-1.7171535, 0.005576049, 1.7172093] d_log=1.7379002571105957\n",
      "iter 1340: g_log=[-1.8094152, 0.0064988057, 1.8094802] d_log=1.784928560256958\n",
      "iter 1350: g_log=[-1.7842529, 0.0056268554, 1.7843091] d_log=1.756333351135254\n",
      "iter 1360: g_log=[-1.8342162, 0.008027392, 1.8342965] d_log=1.7975149154663086\n",
      "iter 1370: g_log=[-1.8080689, 0.0056587113, 1.8081255] d_log=1.780320644378662\n",
      "iter 1380: g_log=[-1.7442384, 0.005789921, 1.7442963] d_log=1.7818224430084229\n",
      "iter 1390: g_log=[-1.8065199, 0.005662909, 1.8065765] d_log=1.7619401216506958\n",
      "iter 1400: g_log=[-1.7884828, 0.004461991, 1.7885274] d_log=1.7747246026992798\n",
      "iter 1410: g_log=[-1.7920231, 0.005780843, 1.7920809] d_log=1.766436219215393\n",
      "iter 1420: g_log=[-1.7489305, 0.0046584015, 1.7489771] d_log=1.7856760025024414\n",
      "iter 1430: g_log=[-1.7669754, 0.0049199886, 1.7670246] d_log=1.7827976942062378\n",
      "iter 1440: g_log=[-1.7221389, 0.005800919, 1.7221969] d_log=1.7879571914672852\n",
      "iter 1450: g_log=[-1.7825892, 0.0049438328, 1.7826387] d_log=1.7906763553619385\n",
      "iter 1460: g_log=[-1.801861, 0.005789532, 1.801919] d_log=1.8056011199951172\n",
      "iter 1470: g_log=[-1.7938963, 0.0049029766, 1.7939453] d_log=1.7446870803833008\n",
      "iter 1480: g_log=[-1.7507677, 0.0056435135, 1.7508241] d_log=1.760911464691162\n",
      "iter 1490: g_log=[-1.8012416, 0.006079033, 1.8013024] d_log=1.782989501953125\n",
      "iter 1500: g_log=[-1.7475487, 0.0050050304, 1.7475988] d_log=1.7761240005493164\n",
      "iter 1510: g_log=[-1.7939625, 0.0052464735, 1.7940149] d_log=1.8034192323684692\n",
      "iter 1520: g_log=[-1.7948635, 0.005439661, 1.7949178] d_log=1.768437385559082\n",
      "iter 1530: g_log=[-1.7944258, 0.005302682, 1.7944789] d_log=1.8027756214141846\n",
      "iter 1540: g_log=[-1.836454, 0.005063761, 1.8365047] d_log=1.7713279724121094\n",
      "iter 1550: g_log=[-1.7422756, 0.006512442, 1.7423407] d_log=1.7535325288772583\n",
      "iter 1560: g_log=[-1.7803444, 0.0049754134, 1.7803941] d_log=1.7754051685333252\n",
      "iter 1570: g_log=[-1.7803663, 0.00468486, 1.7804132] d_log=1.7366905212402344\n",
      "iter 1580: g_log=[-1.7802666, 0.0047739884, 1.7803143] d_log=1.8015373945236206\n",
      "iter 1590: g_log=[-1.7680577, 0.0049915863, 1.7681077] d_log=1.786030888557434\n",
      "iter 1600: g_log=[-1.7971737, 0.0052508907, 1.7972262] d_log=1.7529494762420654\n",
      "iter 1610: g_log=[-1.7649418, 0.004918864, 1.764991] d_log=1.791449785232544\n",
      "iter 1620: g_log=[-1.7911392, 0.0045653884, 1.7911849] d_log=1.759193778038025\n",
      "iter 1630: g_log=[-1.7819024, 0.004705445, 1.7819495] d_log=1.7649580240249634\n",
      "iter 1640: g_log=[-1.8206766, 0.0047207735, 1.8207238] d_log=1.7791707515716553\n",
      "iter 1650: g_log=[-1.7825043, 0.0044837543, 1.7825491] d_log=1.7813680171966553\n",
      "iter 1660: g_log=[-1.7616953, 0.0043356866, 1.7617387] d_log=1.8096177577972412\n",
      "iter 1670: g_log=[-1.7813954, 0.0046551195, 1.7814419] d_log=1.8102238178253174\n",
      "iter 1680: g_log=[-1.7561942, 0.00504749, 1.7562447] d_log=1.7509816884994507\n",
      "iter 1690: g_log=[-1.8285455, 0.00468848, 1.8285923] d_log=1.805246114730835\n",
      "iter 1700: g_log=[-1.8186635, 0.0051668976, 1.8187151] d_log=1.8015198707580566\n",
      "iter 1710: g_log=[-1.7750725, 0.0043464703, 1.775116] d_log=1.8220080137252808\n",
      "iter 1720: g_log=[-1.7456828, 0.0045201173, 1.745728] d_log=1.7967495918273926\n",
      "iter 1730: g_log=[-1.7899656, 0.0047050826, 1.7900127] d_log=1.785080909729004\n",
      "iter 1740: g_log=[-1.8148212, 0.004458707, 1.8148658] d_log=1.7943058013916016\n",
      "iter 1750: g_log=[-1.7939492, 0.004423079, 1.7939935] d_log=1.7686846256256104\n",
      "iter 1760: g_log=[-1.791609, 0.004571711, 1.7916548] d_log=1.7670726776123047\n",
      "iter 1770: g_log=[-1.8352485, 0.0056222877, 1.8353047] d_log=1.823898196220398\n",
      "iter 1780: g_log=[-1.7859429, 0.005110491, 1.785994] d_log=1.7878764867782593\n",
      "iter 1790: g_log=[-1.7758001, 0.004971622, 1.7758498] d_log=1.7750062942504883\n",
      "iter 1800: g_log=[-1.7581782, 0.004350239, 1.7582217] d_log=1.795628547668457\n",
      "iter 1810: g_log=[-1.7555764, 0.0045004897, 1.7556214] d_log=1.7843297719955444\n",
      "iter 1820: g_log=[-1.7806673, 0.005378463, 1.7807211] d_log=1.8152422904968262\n",
      "iter 1830: g_log=[-1.757378, 0.0045312536, 1.7574233] d_log=1.7709510326385498\n",
      "iter 1840: g_log=[-1.7899542, 0.003960753, 1.7899938] d_log=1.7704594135284424\n",
      "iter 1850: g_log=[-1.7677284, 0.0039676, 1.7677681] d_log=1.7711284160614014\n",
      "iter 1860: g_log=[-1.7666496, 0.004038927, 1.76669] d_log=1.7894302606582642\n",
      "iter 1870: g_log=[-1.7998734, 0.004791461, 1.7999213] d_log=1.7770054340362549\n",
      "iter 1880: g_log=[-1.7635987, 0.004729053, 1.763646] d_log=1.7380579710006714\n",
      "iter 1890: g_log=[-1.7866102, 0.004319606, 1.7866534] d_log=1.7711398601531982\n",
      "iter 1900: g_log=[-1.7686685, 0.004117449, 1.7687097] d_log=1.8064168691635132\n",
      "iter 1910: g_log=[-1.7809339, 0.0044845566, 1.7809787] d_log=1.779520034790039\n",
      "iter 1920: g_log=[-1.7771164, 0.003926924, 1.7771556] d_log=1.7877838611602783\n",
      "iter 1930: g_log=[-1.7423717, 0.0048050284, 1.7424197] d_log=1.791965365409851\n",
      "iter 1940: g_log=[-1.7727757, 0.0045472467, 1.7728211] d_log=1.7793102264404297\n",
      "iter 1950: g_log=[-1.8146266, 0.0048043765, 1.8146746] d_log=1.766947627067566\n",
      "iter 1960: g_log=[-1.7865443, 0.00422294, 1.7865865] d_log=1.7769641876220703\n",
      "iter 1970: g_log=[-1.7798553, 0.004219883, 1.7798975] d_log=1.7897584438323975\n",
      "iter 1980: g_log=[-1.7835664, 0.0040607867, 1.783607] d_log=1.7716355323791504\n",
      "iter 1990: g_log=[-1.8098091, 0.004250765, 1.8098516] d_log=1.7722433805465698\n",
      "iter 2000: g_log=[-1.7886707, 0.0043307487, 1.7887139] d_log=1.785304069519043\n",
      "iter 2010: g_log=[-1.7897503, 0.0037215408, 1.7897875] d_log=1.7721331119537354\n",
      "iter 2020: g_log=[-1.8020262, 0.004159717, 1.8020678] d_log=1.8092079162597656\n",
      "iter 2030: g_log=[-1.7586055, 0.004484008, 1.7586503] d_log=1.7906687259674072\n",
      "iter 2040: g_log=[-1.7585243, 0.0038421277, 1.7585627] d_log=1.8280704021453857\n",
      "iter 2050: g_log=[-1.7951155, 0.004812296, 1.7951636] d_log=1.7863978147506714\n",
      "iter 2060: g_log=[-1.8100837, 0.004227088, 1.8101261] d_log=1.7849911451339722\n",
      "iter 2070: g_log=[-1.7439636, 0.0041369447, 1.744005] d_log=1.817158818244934\n",
      "iter 2080: g_log=[-1.7799668, 0.0040512905, 1.7800074] d_log=1.7685214281082153\n",
      "iter 2090: g_log=[-1.8113883, 0.0040026465, 1.8114283] d_log=1.8203461170196533\n",
      "iter 2100: g_log=[-1.7769582, 0.004080291, 1.776999] d_log=1.7967288494110107\n",
      "iter 2110: g_log=[-1.8036233, 0.004301599, 1.8036664] d_log=1.7624683380126953\n",
      "iter 2120: g_log=[-1.7887218, 0.0038444838, 1.7887602] d_log=1.7711236476898193\n",
      "iter 2130: g_log=[-1.7686384, 0.0041405805, 1.7686797] d_log=1.8050377368927002\n",
      "iter 2140: g_log=[-1.7841334, 0.004745816, 1.7841809] d_log=1.8003015518188477\n",
      "iter 2150: g_log=[-1.7972782, 0.0037843613, 1.797316] d_log=1.795053243637085\n",
      "iter 2160: g_log=[-1.821271, 0.0041764127, 1.8213127] d_log=1.8091511726379395\n",
      "iter 2170: g_log=[-1.780246, 0.0037661497, 1.7802837] d_log=1.7796964645385742\n",
      "iter 2180: g_log=[-1.7841072, 0.0037745684, 1.784145] d_log=1.7543386220932007\n",
      "iter 2190: g_log=[-1.7655326, 0.0043849368, 1.7655765] d_log=1.7527573108673096\n",
      "iter 2200: g_log=[-1.7641985, 0.003935449, 1.7642379] d_log=1.8186395168304443\n",
      "iter 2210: g_log=[-1.7744075, 0.003678278, 1.7744443] d_log=1.7629923820495605\n",
      "iter 2220: g_log=[-1.8336099, 0.0043535605, 1.8336535] d_log=1.7747490406036377\n",
      "iter 2230: g_log=[-1.8188492, 0.0039211456, 1.8188884] d_log=1.7782764434814453\n",
      "iter 2240: g_log=[-1.7753713, 0.0040043993, 1.7754114] d_log=1.7556266784667969\n",
      "iter 2250: g_log=[-1.7912431, 0.003552028, 1.7912786] d_log=1.794004201889038\n",
      "iter 2260: g_log=[-1.7786256, 0.00323013, 1.7786579] d_log=1.788197636604309\n",
      "iter 2270: g_log=[-1.7951907, 0.003965321, 1.7952304] d_log=1.7850675582885742\n",
      "iter 2280: g_log=[-1.8030701, 0.0044084266, 1.8031142] d_log=1.7750511169433594\n",
      "iter 2290: g_log=[-1.7996451, 0.0037676916, 1.7996827] d_log=1.7882068157196045\n",
      "iter 2300: g_log=[-1.7578675, 0.0042220666, 1.7579097] d_log=1.804978847503662\n",
      "iter 2310: g_log=[-1.7449726, 0.0037335574, 1.7450099] d_log=1.7558363676071167\n",
      "iter 2320: g_log=[-1.7781298, 0.0047770524, 1.7781776] d_log=1.765268087387085\n",
      "iter 2330: g_log=[-1.7554275, 0.0041662804, 1.7554691] d_log=1.7928965091705322\n",
      "iter 2340: g_log=[-1.781371, 0.0040351944, 1.7814113] d_log=1.7971200942993164\n",
      "iter 2350: g_log=[-1.7914641, 0.003767991, 1.7915018] d_log=1.7528283596038818\n",
      "iter 2360: g_log=[-1.778521, 0.0037571648, 1.7785585] d_log=1.7694042921066284\n",
      "iter 2370: g_log=[-1.7669303, 0.0036867298, 1.7669672] d_log=1.7643067836761475\n",
      "iter 2380: g_log=[-1.7737963, 0.0039862245, 1.7738361] d_log=1.798799991607666\n",
      "iter 2390: g_log=[-1.7707886, 0.003763578, 1.7708262] d_log=1.7802822589874268\n",
      "iter 2400: g_log=[-1.7877829, 0.0038670131, 1.7878215] d_log=1.7944873571395874\n",
      "iter 2410: g_log=[-1.7935586, 0.0040961923, 1.7935996] d_log=1.7635087966918945\n",
      "iter 2420: g_log=[-1.764183, 0.003985408, 1.7642229] d_log=1.7938587665557861\n",
      "iter 2430: g_log=[-1.7716538, 0.0045278403, 1.7716991] d_log=1.7999308109283447\n",
      "iter 2440: g_log=[-1.7540299, 0.0040154783, 1.75407] d_log=1.7767503261566162\n",
      "iter 2450: g_log=[-1.7742825, 0.0044184197, 1.7743267] d_log=1.7047083377838135\n",
      "iter 2460: g_log=[-1.7960589, 0.0033199242, 1.796092] d_log=1.7830860614776611\n",
      "iter 2470: g_log=[-1.7480814, 0.003938187, 1.7481208] d_log=1.805631160736084\n",
      "iter 2480: g_log=[-1.7624454, 0.004290476, 1.7624884] d_log=1.7451705932617188\n",
      "iter 2490: g_log=[-1.7579352, 0.0036537838, 1.7579718] d_log=1.795508623123169\n",
      "iter 2500: g_log=[-1.7889818, 0.0038280706, 1.7890201] d_log=1.7736473083496094\n",
      "iter 2510: g_log=[-1.7850019, 0.0035658986, 1.7850375] d_log=1.8028686046600342\n",
      "iter 2520: g_log=[-1.7413542, 0.003748124, 1.7413917] d_log=1.7819569110870361\n",
      "iter 2530: g_log=[-1.8091012, 0.0039412146, 1.8091407] d_log=1.7757244110107422\n",
      "iter 2540: g_log=[-1.7723613, 0.0034097456, 1.7723954] d_log=1.7806026935577393\n",
      "iter 2550: g_log=[-1.8079498, 0.0043015447, 1.8079928] d_log=1.7687461376190186\n",
      "iter 2560: g_log=[-1.7863568, 0.0043863985, 1.7864007] d_log=1.776435375213623\n",
      "iter 2570: g_log=[-1.7810457, 0.0043431204, 1.7810891] d_log=1.7888603210449219\n",
      "iter 2580: g_log=[-1.8065172, 0.004540466, 1.8065627] d_log=1.8069472312927246\n",
      "iter 2590: g_log=[-1.7955205, 0.0039543267, 1.7955601] d_log=1.7857543230056763\n",
      "iter 2600: g_log=[-1.7994775, 0.003878294, 1.7995162] d_log=1.761285424232483\n",
      "iter 2610: g_log=[-1.7772257, 0.004307544, 1.7772688] d_log=1.799063801765442\n",
      "iter 2620: g_log=[-1.7784622, 0.004317654, 1.7785053] d_log=1.7690303325653076\n",
      "iter 2630: g_log=[-1.7750548, 0.0038311307, 1.7750931] d_log=1.7719924449920654\n",
      "iter 2640: g_log=[-1.7977148, 0.0037977938, 1.7977529] d_log=1.7990615367889404\n",
      "iter 2650: g_log=[-1.7958568, 0.0036189575, 1.7958931] d_log=1.7689439058303833\n",
      "iter 2660: g_log=[-1.7977033, 0.0038370332, 1.7977417] d_log=1.7974097728729248\n",
      "iter 2670: g_log=[-1.7536576, 0.00359408, 1.7536935] d_log=1.798813819885254\n",
      "iter 2680: g_log=[-1.7780839, 0.0038399566, 1.7781223] d_log=1.76348876953125\n",
      "iter 2690: g_log=[-1.7668256, 0.004426766, 1.7668698] d_log=1.7520604133605957\n",
      "iter 2700: g_log=[-1.7611816, 0.004101152, 1.7612226] d_log=1.7865917682647705\n",
      "iter 2710: g_log=[-1.7933564, 0.0037221722, 1.7933936] d_log=1.7586828470230103\n",
      "iter 2720: g_log=[-1.7771403, 0.0039933682, 1.7771802] d_log=1.790452003479004\n",
      "iter 2730: g_log=[-1.8027933, 0.0039111683, 1.8028324] d_log=1.785107970237732\n",
      "iter 2740: g_log=[-1.7888688, 0.0037049113, 1.7889059] d_log=1.7776798009872437\n",
      "iter 2750: g_log=[-1.7548623, 0.0035110577, 1.7548975] d_log=1.7924139499664307\n",
      "iter 2760: g_log=[-1.7352631, 0.0036770455, 1.7352998] d_log=1.7917042970657349\n",
      "iter 2770: g_log=[-1.7690239, 0.0036971332, 1.7690609] d_log=1.786506175994873\n",
      "iter 2780: g_log=[-1.7893562, 0.0039986894, 1.7893962] d_log=1.7782996892929077\n",
      "iter 2790: g_log=[-1.7953966, 0.0036531177, 1.795433] d_log=1.7811903953552246\n",
      "iter 2800: g_log=[-1.7694719, 0.004081097, 1.7695127] d_log=1.7735264301300049\n",
      "iter 2810: g_log=[-1.7905658, 0.003747784, 1.7906033] d_log=1.7791502475738525\n",
      "iter 2820: g_log=[-1.7842004, 0.0036238385, 1.7842367] d_log=1.7659310102462769\n",
      "iter 2830: g_log=[-1.7781559, 0.0036174627, 1.778192] d_log=1.7736289501190186\n",
      "iter 2840: g_log=[-1.7716539, 0.0035148766, 1.771689] d_log=1.7574280500411987\n",
      "iter 2850: g_log=[-1.7456671, 0.0032326418, 1.7456994] d_log=1.7757635116577148\n",
      "iter 2860: g_log=[-1.7813077, 0.0041715037, 1.7813494] d_log=1.7713440656661987\n",
      "iter 2870: g_log=[-1.7429086, 0.003964721, 1.7429483] d_log=1.7466740608215332\n",
      "iter 2880: g_log=[-1.806081, 0.0034689517, 1.8061157] d_log=1.8170843124389648\n",
      "iter 2890: g_log=[-1.787528, 0.0036247037, 1.7875643] d_log=1.7836569547653198\n",
      "iter 2900: g_log=[-1.7662852, 0.0036414256, 1.7663215] d_log=1.8004701137542725\n",
      "iter 2910: g_log=[-1.7801335, 0.003405318, 1.7801676] d_log=1.7725697755813599\n",
      "iter 2920: g_log=[-1.8007227, 0.00361459, 1.8007588] d_log=1.7592363357543945\n",
      "iter 2930: g_log=[-1.7828454, 0.0037116169, 1.7828825] d_log=1.763200283050537\n",
      "iter 2940: g_log=[-1.7875193, 0.003874061, 1.7875581] d_log=1.788959264755249\n",
      "iter 2950: g_log=[-1.76921, 0.004048705, 1.7692505] d_log=1.7874165773391724\n",
      "iter 2960: g_log=[-1.7912538, 0.0038588224, 1.7912924] d_log=1.7912101745605469\n",
      "iter 2970: g_log=[-1.7907873, 0.0035641533, 1.790823] d_log=1.7654818296432495\n",
      "iter 2980: g_log=[-1.8172822, 0.0039484855, 1.8173217] d_log=1.791762351989746\n",
      "iter 2990: g_log=[-1.8053505, 0.0035964136, 1.8053865] d_log=1.8129873275756836\n",
      "iter 3000: g_log=[-1.7666594, 0.0035607836, 1.766695] d_log=1.7771251201629639\n",
      "iter 3010: g_log=[-1.7689546, 0.003493951, 1.7689896] d_log=1.7698125839233398\n",
      "iter 3020: g_log=[-1.7544874, 0.003346971, 1.7545209] d_log=1.7817271947860718\n",
      "iter 3030: g_log=[-1.8035176, 0.0032852597, 1.8035505] d_log=1.8007183074951172\n",
      "iter 3040: g_log=[-1.7797676, 0.0031532159, 1.7797992] d_log=1.7945055961608887\n",
      "iter 3050: g_log=[-1.760152, 0.0038673163, 1.7601906] d_log=1.753393530845642\n",
      "iter 3060: g_log=[-1.7811538, 0.003547295, 1.7811893] d_log=1.7752230167388916\n",
      "iter 3070: g_log=[-1.8166652, 0.0035561211, 1.8167007] d_log=1.8209120035171509\n",
      "iter 3080: g_log=[-1.7977728, 0.0033234027, 1.797806] d_log=1.7849695682525635\n",
      "iter 3090: g_log=[-1.8002955, 0.0035600006, 1.8003311] d_log=1.8134950399398804\n",
      "iter 3100: g_log=[-1.7690613, 0.0036866982, 1.7690982] d_log=1.802614688873291\n",
      "iter 3110: g_log=[-1.8070347, 0.0037889928, 1.8070726] d_log=1.7865207195281982\n",
      "iter 3120: g_log=[-1.8047128, 0.0035554534, 1.8047483] d_log=1.7801235914230347\n",
      "iter 3130: g_log=[-1.7695508, 0.00357136, 1.7695866] d_log=1.7693867683410645\n",
      "iter 3140: g_log=[-1.7568164, 0.0038832163, 1.7568552] d_log=1.7865123748779297\n",
      "iter 3150: g_log=[-1.7696731, 0.004225484, 1.7697153] d_log=1.7922189235687256\n",
      "iter 3160: g_log=[-1.7781874, 0.0035844138, 1.7782233] d_log=1.7563176155090332\n",
      "iter 3170: g_log=[-1.777823, 0.0030812426, 1.7778537] d_log=1.7991856336593628\n",
      "iter 3180: g_log=[-1.7770886, 0.0038950972, 1.7771276] d_log=1.7738596200942993\n",
      "iter 3190: g_log=[-1.754597, 0.0045272573, 1.7546422] d_log=1.7885264158248901\n",
      "iter 3200: g_log=[-1.8040323, 0.003619826, 1.8040686] d_log=1.7780640125274658\n",
      "iter 3210: g_log=[-1.7905159, 0.0038003526, 1.7905539] d_log=1.765640377998352\n",
      "iter 3220: g_log=[-1.7964951, 0.0037979775, 1.7965331] d_log=1.7994270324707031\n",
      "iter 3230: g_log=[-1.816213, 0.0039014507, 1.816252] d_log=1.7718952894210815\n",
      "iter 3240: g_log=[-1.7951884, 0.0031190512, 1.7952197] d_log=1.7863428592681885\n",
      "iter 3250: g_log=[-1.7556386, 0.0036332502, 1.755675] d_log=1.767578125\n",
      "iter 3260: g_log=[-1.7915671, 0.003205108, 1.7915992] d_log=1.784844160079956\n",
      "iter 3270: g_log=[-1.7852972, 0.0033663255, 1.7853308] d_log=1.8028932809829712\n",
      "iter 3280: g_log=[-1.7969105, 0.0037920699, 1.7969484] d_log=1.7871894836425781\n",
      "iter 3290: g_log=[-1.7580533, 0.003113913, 1.7580844] d_log=1.8075169324874878\n",
      "iter 3300: g_log=[-1.7724508, 0.003432054, 1.7724851] d_log=1.7662878036499023\n",
      "iter 3310: g_log=[-1.7668741, 0.004051562, 1.7669146] d_log=1.7906122207641602\n",
      "iter 3320: g_log=[-1.7679608, 0.0034060078, 1.7679949] d_log=1.7747503519058228\n",
      "iter 3330: g_log=[-1.7908627, 0.0042751255, 1.7909055] d_log=1.763200044631958\n",
      "iter 3340: g_log=[-1.7921903, 0.003336574, 1.7922237] d_log=1.8129773139953613\n",
      "iter 3350: g_log=[-1.796521, 0.003358018, 1.7965546] d_log=1.78814697265625\n",
      "iter 3360: g_log=[-1.8020867, 0.0046125026, 1.8021328] d_log=1.7703830003738403\n",
      "iter 3370: g_log=[-1.778578, 0.0038844068, 1.7786169] d_log=1.7603819370269775\n",
      "iter 3380: g_log=[-1.7941017, 0.0037928587, 1.7941396] d_log=1.8047184944152832\n",
      "iter 3390: g_log=[-1.7660673, 0.003555026, 1.7661028] d_log=1.7453935146331787\n",
      "iter 3400: g_log=[-1.7913367, 0.0031907703, 1.7913686] d_log=1.8024846315383911\n",
      "iter 3410: g_log=[-1.7732952, 0.0033153538, 1.7733283] d_log=1.7890815734863281\n",
      "iter 3420: g_log=[-1.769047, 0.0034811161, 1.7690818] d_log=1.7586064338684082\n",
      "iter 3430: g_log=[-1.7687153, 0.003568036, 1.7687509] d_log=1.7951170206069946\n",
      "iter 3440: g_log=[-1.8125628, 0.0042335247, 1.8126051] d_log=1.824949026107788\n",
      "iter 3450: g_log=[-1.8019567, 0.0037223448, 1.8019938] d_log=1.7755377292633057\n",
      "iter 3460: g_log=[-1.7629241, 0.0032627177, 1.7629567] d_log=1.759063720703125\n",
      "iter 3470: g_log=[-1.7920979, 0.003164729, 1.7921295] d_log=1.753268837928772\n",
      "iter 3480: g_log=[-1.7940797, 0.004018099, 1.7941198] d_log=1.7740650177001953\n",
      "iter 3490: g_log=[-1.7793386, 0.0037106834, 1.7793757] d_log=1.7671419382095337\n",
      "iter 3500: g_log=[-1.7682239, 0.0032332567, 1.7682562] d_log=1.777529239654541\n",
      "iter 3510: g_log=[-1.7656735, 0.0037293374, 1.7657108] d_log=1.7480369806289673\n",
      "iter 3520: g_log=[-1.7860506, 0.0030995775, 1.7860816] d_log=1.7648496627807617\n",
      "iter 3530: g_log=[-1.7887717, 0.0033176253, 1.7888049] d_log=1.8055603504180908\n",
      "iter 3540: g_log=[-1.7499404, 0.0030573145, 1.7499709] d_log=1.7727569341659546\n",
      "iter 3550: g_log=[-1.7709975, 0.0035323899, 1.7710328] d_log=1.8092453479766846\n",
      "iter 3560: g_log=[-1.7848384, 0.0036311722, 1.7848748] d_log=1.7568111419677734\n",
      "iter 3570: g_log=[-1.7598158, 0.0030662876, 1.7598464] d_log=1.75710129737854\n",
      "iter 3580: g_log=[-1.7838093, 0.0032056265, 1.7838414] d_log=1.758426308631897\n",
      "iter 3590: g_log=[-1.753335, 0.0035355543, 1.7533704] d_log=1.7498975992202759\n",
      "iter 3600: g_log=[-1.7775242, 0.0032543517, 1.7775568] d_log=1.784332036972046\n",
      "iter 3610: g_log=[-1.7896659, 0.0035013906, 1.789701] d_log=1.757086992263794\n",
      "iter 3620: g_log=[-1.811255, 0.0038775962, 1.8112937] d_log=1.774308443069458\n",
      "iter 3630: g_log=[-1.7961675, 0.003797367, 1.7962055] d_log=1.7815258502960205\n",
      "iter 3640: g_log=[-1.7838525, 0.003377135, 1.7838862] d_log=1.7523326873779297\n",
      "iter 3650: g_log=[-1.7660116, 0.0035933405, 1.7660475] d_log=1.7924320697784424\n",
      "iter 3660: g_log=[-1.8033421, 0.00331293, 1.8033752] d_log=1.763206958770752\n",
      "iter 3670: g_log=[-1.7879851, 0.0036182534, 1.7880213] d_log=1.7642749547958374\n",
      "iter 3680: g_log=[-1.7852596, 0.0034445853, 1.785294] d_log=1.7965924739837646\n",
      "iter 3690: g_log=[-1.7759526, 0.0031657126, 1.7759843] d_log=1.7475194931030273\n",
      "iter 3700: g_log=[-1.778652, 0.0039784964, 1.7786918] d_log=1.7670964002609253\n",
      "iter 3710: g_log=[-1.788075, 0.0037845995, 1.7881128] d_log=1.812550663948059\n",
      "iter 3720: g_log=[-1.7754062, 0.0029614086, 1.7754358] d_log=1.733156442642212\n",
      "iter 3730: g_log=[-1.795398, 0.003432924, 1.7954323] d_log=1.7737958431243896\n",
      "iter 3740: g_log=[-1.7868148, 0.0030427186, 1.7868452] d_log=1.7925924062728882\n",
      "iter 3750: g_log=[-1.7905211, 0.00403343, 1.7905614] d_log=1.7604994773864746\n",
      "iter 3760: g_log=[-1.7745945, 0.003279399, 1.7746273] d_log=1.8125340938568115\n",
      "iter 3770: g_log=[-1.7676725, 0.0032202161, 1.7677047] d_log=1.788914442062378\n",
      "iter 3780: g_log=[-1.7481865, 0.004182154, 1.7482283] d_log=1.776440143585205\n",
      "iter 3790: g_log=[-1.8059456, 0.0038642506, 1.8059843] d_log=1.7554066181182861\n",
      "iter 3800: g_log=[-1.7791917, 0.003292205, 1.7792246] d_log=1.7460860013961792\n",
      "iter 3810: g_log=[-1.7817945, 0.0033847343, 1.7818284] d_log=1.7775845527648926\n",
      "iter 3820: g_log=[-1.812107, 0.0034637065, 1.8121417] d_log=1.788651943206787\n",
      "iter 3830: g_log=[-1.8091528, 0.0038012692, 1.8091909] d_log=1.7584702968597412\n",
      "iter 3840: g_log=[-1.7730578, 0.0031825404, 1.7730896] d_log=1.7625460624694824\n",
      "iter 3850: g_log=[-1.7740777, 0.003078504, 1.7741084] d_log=1.7907160520553589\n",
      "iter 3860: g_log=[-1.7343283, 0.0033333828, 1.7343616] d_log=1.7983229160308838\n",
      "iter 3870: g_log=[-1.7546555, 0.002753005, 1.754683] d_log=1.763993740081787\n",
      "iter 3880: g_log=[-1.7954184, 0.003107652, 1.7954495] d_log=1.803278923034668\n",
      "iter 3890: g_log=[-1.7890742, 0.0029300735, 1.7891035] d_log=1.7466583251953125\n",
      "iter 3900: g_log=[-1.7867621, 0.003032394, 1.7867924] d_log=1.776526689529419\n",
      "iter 3910: g_log=[-1.7891983, 0.0031979964, 1.7892302] d_log=1.792832612991333\n",
      "iter 3920: g_log=[-1.8110294, 0.0034532885, 1.811064] d_log=1.792086124420166\n",
      "iter 3930: g_log=[-1.7772812, 0.0033119982, 1.7773143] d_log=1.7862566709518433\n",
      "iter 3940: g_log=[-1.822067, 0.0036227866, 1.8221033] d_log=1.754201889038086\n",
      "iter 3950: g_log=[-1.7841096, 0.0034877323, 1.7841445] d_log=1.7129595279693604\n",
      "iter 3960: g_log=[-1.8179054, 0.0030709757, 1.8179362] d_log=1.7378469705581665\n",
      "iter 3970: g_log=[-1.7843652, 0.003688171, 1.784402] d_log=1.7792387008666992\n",
      "iter 3980: g_log=[-1.7778434, 0.003139129, 1.7778747] d_log=1.7670891284942627\n",
      "iter 3990: g_log=[-1.7519121, 0.002980569, 1.7519419] d_log=1.7777116298675537\n",
      "iter 4000: g_log=[-1.7774088, 0.0029305723, 1.7774382] d_log=1.795177936553955\n",
      "iter 4010: g_log=[-1.791947, 0.0030144819, 1.7919772] d_log=1.776854395866394\n",
      "iter 4020: g_log=[-1.802287, 0.0032523829, 1.8023195] d_log=1.8011674880981445\n",
      "iter 4030: g_log=[-1.8196994, 0.0035125967, 1.8197346] d_log=1.7764801979064941\n",
      "iter 4040: g_log=[-1.7835557, 0.0033602975, 1.7835894] d_log=1.8008289337158203\n",
      "iter 4050: g_log=[-1.8129691, 0.0037552142, 1.8130066] d_log=1.7644455432891846\n",
      "iter 4060: g_log=[-1.8001119, 0.003401179, 1.8001459] d_log=1.786175012588501\n",
      "iter 4070: g_log=[-1.7971916, 0.0037858933, 1.7972295] d_log=1.766290545463562\n",
      "iter 4080: g_log=[-1.8004625, 0.0034619558, 1.800497] d_log=1.7848904132843018\n",
      "iter 4090: g_log=[-1.7675227, 0.002852585, 1.7675512] d_log=1.788022756576538\n",
      "iter 4100: g_log=[-1.8146688, 0.003538032, 1.8147042] d_log=1.7703232765197754\n",
      "iter 4110: g_log=[-1.787324, 0.0034610655, 1.7873585] d_log=1.801650047302246\n",
      "iter 4120: g_log=[-1.8079315, 0.003709864, 1.8079686] d_log=1.7518506050109863\n",
      "iter 4130: g_log=[-1.7565712, 0.0031871798, 1.756603] d_log=1.7787604331970215\n",
      "iter 4140: g_log=[-1.7878339, 0.0034124518, 1.787868] d_log=1.7966444492340088\n",
      "iter 4150: g_log=[-1.7653872, 0.003369225, 1.7654209] d_log=1.7889854907989502\n",
      "iter 4160: g_log=[-1.7511625, 0.0033883636, 1.7511964] d_log=1.7716186046600342\n",
      "iter 4170: g_log=[-1.7515726, 0.003230601, 1.7516049] d_log=1.7979021072387695\n",
      "iter 4180: g_log=[-1.7784983, 0.003157707, 1.7785299] d_log=1.7669355869293213\n",
      "iter 4190: g_log=[-1.7892555, 0.0032255652, 1.7892878] d_log=1.7855663299560547\n",
      "iter 4200: g_log=[-1.8060387, 0.003208503, 1.8060708] d_log=1.7906556129455566\n",
      "iter 4210: g_log=[-1.7936957, 0.0031002853, 1.7937267] d_log=1.7614243030548096\n",
      "iter 4220: g_log=[-1.7651383, 0.0031854468, 1.7651701] d_log=1.8018720149993896\n",
      "iter 4230: g_log=[-1.7909158, 0.003518371, 1.790951] d_log=1.7534805536270142\n",
      "iter 4240: g_log=[-1.8015474, 0.0028783516, 1.8015761] d_log=1.7195980548858643\n",
      "iter 4250: g_log=[-1.7655932, 0.003371903, 1.7656269] d_log=1.8014585971832275\n",
      "iter 4260: g_log=[-1.7762076, 0.0028180676, 1.7762357] d_log=1.787654161453247\n",
      "iter 4270: g_log=[-1.7791651, 0.002933383, 1.7791945] d_log=1.8062992095947266\n",
      "iter 4280: g_log=[-1.750216, 0.0034118593, 1.7502501] d_log=1.7689152956008911\n",
      "iter 4290: g_log=[-1.7464592, 0.003268612, 1.7464919] d_log=1.7589031457901\n",
      "iter 4300: g_log=[-1.798817, 0.002991182, 1.798847] d_log=1.7952206134796143\n",
      "iter 4310: g_log=[-1.7884448, 0.0027678378, 1.7884724] d_log=1.790755033493042\n",
      "iter 4320: g_log=[-1.7918823, 0.002733402, 1.7919096] d_log=1.786534070968628\n",
      "iter 4330: g_log=[-1.7427093, 0.0032812555, 1.7427421] d_log=1.7806205749511719\n",
      "iter 4340: g_log=[-1.7917649, 0.0031821528, 1.7917967] d_log=1.777750015258789\n",
      "iter 4350: g_log=[-1.8047129, 0.0034162716, 1.8047471] d_log=1.7663929462432861\n",
      "iter 4360: g_log=[-1.7568216, 0.0037506137, 1.7568592] d_log=1.7721631526947021\n",
      "iter 4370: g_log=[-1.8122362, 0.003697067, 1.8122731] d_log=1.7836873531341553\n",
      "iter 4380: g_log=[-1.7570134, 0.0027502303, 1.757041] d_log=1.7603352069854736\n",
      "iter 4390: g_log=[-1.7785925, 0.002738759, 1.7786199] d_log=1.773047924041748\n",
      "iter 4400: g_log=[-1.7751781, 0.0032803374, 1.7752109] d_log=1.7614299058914185\n",
      "iter 4410: g_log=[-1.7526051, 0.0029205605, 1.7526343] d_log=1.8053979873657227\n",
      "iter 4420: g_log=[-1.8294829, 0.0034602082, 1.8295175] d_log=1.7714077234268188\n",
      "iter 4430: g_log=[-1.7514516, 0.0036566309, 1.7514882] d_log=1.7809665203094482\n",
      "iter 4440: g_log=[-1.7792909, 0.003104827, 1.7793219] d_log=1.768966794013977\n",
      "iter 4450: g_log=[-1.776531, 0.0029806807, 1.7765608] d_log=1.7860321998596191\n",
      "iter 4460: g_log=[-1.7762698, 0.0030431119, 1.7763002] d_log=1.740940809249878\n"
     ]
    }
   ],
   "source": [
    "class Adae:\n",
    "    def __init__(self,\n",
    "                 img_dim=64,\n",
    "                 z_dim=128\n",
    "                 ):\n",
    "        def build_encoder():\n",
    "            x_in = Input(shape=(img_dim, img_dim, 3))\n",
    "            x = x_in\n",
    "            for i in range(3):\n",
    "                x = Conv2D(z_dim // 2**(2-i),\n",
    "                           kernel_size=(3,3),\n",
    "                           padding='SAME')(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = LeakyReLU(0.2)(x)\n",
    "                x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "            feature_map = x # 截断到这里，认为到这里是feature_map（局部特征）\n",
    "            feature_map_encoder = Model(x_in, x)\n",
    "\n",
    "\n",
    "            for i in range(2):\n",
    "                x = Conv2D(z_dim,\n",
    "                           kernel_size=(3,3),\n",
    "                           padding='SAME')(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = LeakyReLU(0.2)(x)\n",
    "\n",
    "            x = GlobalMaxPooling2D()(x) # 全局特征\n",
    "\n",
    "            z = Dense(z_dim)(x) # 均值，也就是最终输出的编码\n",
    "            return Model(x_in, z)\n",
    "        \n",
    "        def build_decoder():\n",
    "            z_in = Input(shape=(z_dim,))\n",
    "            k = 8\n",
    "            units = 128\n",
    "            h = z_in\n",
    "            h = Dense(4 * 4 * 128, activation='relu')(h)\n",
    "            h = Reshape((4, 4, 128))(h)\n",
    "            # h = LeakyReLU(0.2)(h)\n",
    "            h = Conv2DTranspose(units, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 32*32*64\n",
    "            # h = Dropout(dropout)(h)\n",
    "            h = BatchNormalization(momentum=0.8)(h)\n",
    "            # h = LeakyReLU(0.2)(h)\n",
    "            # h = UpSampling2D(size=(2, 2))(h)\n",
    "            h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 64*64*64\n",
    "            # h = Dropout(dropout)(h)\n",
    "            h = BatchNormalization(momentum=0.8)(h)\n",
    "            # h = LeakyReLU(0.2)(h)\n",
    "            # h = UpSampling2D(size=(2, 2))(h)\n",
    "            h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 8*6*64\n",
    "            # h = Dropout(dropout)(h)\n",
    "            h = BatchNormalization(momentum=0.8)(h)\n",
    "            h = Conv2DTranspose(3, (k, k), strides=(2, 2), padding='same', activation='tanh')(h)  # 8*6*64\n",
    "            return Model(z_in, h)\n",
    "            \n",
    "        def build_discrim():\n",
    "            x_in = Input(shape=(z_dim,))\n",
    "            x = x_in\n",
    "            x = Dense(z_dim, activation='relu')(x)\n",
    "            x = Dense(6, activation='softmax')(x)\n",
    "            return Model(x_in, x)\n",
    "         \n",
    "        encoder = build_encoder()\n",
    "        decoder = build_decoder()\n",
    "        classifier = build_discrim()\n",
    "        x_in = Input(shape=(img_dim, img_dim, 3))\n",
    "        z = encoder(x_in)\n",
    "        rec_x = decoder(z)\n",
    "        rec_x = Lambda(lambda x: x, name=\"rec_x\")(rec_x)\n",
    "        p_pred = classifier(z)\n",
    "        p_pred = Lambda(lambda x: x, name=\"p\")(p_pred)\n",
    "        \n",
    "        #build g_train_model\n",
    "        g_train_model = Model(x_in, [rec_x, p_pred])\n",
    "        encoder.trainable = True\n",
    "        decoder.trainable = True\n",
    "        classifier.trainable = False\n",
    "        g_train_model.compile(optimizer=Adam(1e-3),\n",
    "                              loss={'p': 'categorical_crossentropy', 'rec_x': 'mean_squared_error'},\n",
    "                              loss_weights={'p':-1, 'rec_x':0.01})\n",
    "        \n",
    "        #build d_train_model\n",
    "        d_train_model = Model(x_in, p_pred)\n",
    "        encoder.trainable = False\n",
    "        decoder.trainable = False\n",
    "        classifier.trainable = True\n",
    "        d_train_model.compile(optimizer=Adam(1e-3),\n",
    "                              loss={'p': 'categorical_crossentropy'})\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.g_train_model = g_train_model\n",
    "        self.d_train_model = d_train_model\n",
    "        \n",
    "    def train_d(self, batch_data):\n",
    "        x = batch_data['x']\n",
    "        p = batch_data['p']\n",
    "        loss = self.d_train_model.train_on_batch(x, y={'p': p})\n",
    "        return loss\n",
    "    def train_g(self, batch_data):\n",
    "        x = batch_data['x']\n",
    "        p = batch_data['p']\n",
    "        loss = self.g_train_model.train_on_batch(x, y={'p':p, 'rec_x':x})\n",
    "        return loss\n",
    "\n",
    "privater = Adae()\n",
    "trainer = PrivaterTrainer()\n",
    "trainer.train(ferg, privater=privater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255 - 0.5\n",
    "x_test = x_test.astype('float32') / 255 - 0.5\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "img_dim = x_train.shape[1]\n",
    "\n",
    "\n",
    "z_dim = 256 # 隐变量维度\n",
    "alpha = 0.5 # 全局互信息的loss比重\n",
    "beta = 1.5 # 局部互信息的loss比重\n",
    "gamma = 0.01 # 先验分布的loss比重\n",
    "\n",
    "\n",
    "# 编码器（卷积与最大池化）\n",
    "x_in = Input(shape=(img_dim, img_dim, 3))\n",
    "x = x_in\n",
    "\n",
    "for i in range(3):\n",
    "    x = Conv2D(z_dim / 2**(2-i),\n",
    "               kernel_size=(3,3),\n",
    "               padding='SAME')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "feature_map = x # 截断到这里，认为到这里是feature_map（局部特征）\n",
    "feature_map_encoder = Model(x_in, x)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    x = Conv2D(z_dim,\n",
    "               kernel_size=(3,3),\n",
    "               padding='SAME')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "x = GlobalMaxPooling2D()(x) # 全局特征\n",
    "\n",
    "z_mean = Dense(z_dim)(x) # 均值，也就是最终输出的编码\n",
    "z_log_var = Dense(z_dim)(x) # 方差，这里都是模仿VAE的\n",
    "\n",
    "\n",
    "encoder = Model(x_in, z_mean) # 总的编码器就是输出z_mean\n",
    "\n",
    "\n",
    "# 重参数技巧\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    u = K.random_normal(shape=K.shape(z_mean))\n",
    "    return z_mean + K.exp(z_log_var / 2) * u\n",
    "\n",
    "\n",
    "# 重参数层，相当于给输入加入噪声\n",
    "z_samples = Lambda(sampling)([z_mean, z_log_var])\n",
    "prior_kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "\n",
    "\n",
    "# shuffle层，打乱第一个轴\n",
    "def shuffling(x):\n",
    "    idxs = K.arange(0, K.shape(x)[0])\n",
    "    idxs = K.tf.random_shuffle(idxs)\n",
    "    return K.gather(x, idxs)\n",
    "\n",
    "\n",
    "# 与随机采样的特征拼接（全局）\n",
    "z_shuffle = Lambda(shuffling)(z_samples)\n",
    "z_z_1 = Concatenate()([z_samples, z_samples])\n",
    "z_z_2 = Concatenate()([z_samples, z_shuffle])\n",
    "\n",
    "# 与随机采样的特征拼接（局部）\n",
    "feature_map_shuffle = Lambda(shuffling)(feature_map)\n",
    "z_samples_repeat = RepeatVector(4 * 4)(z_samples)\n",
    "z_samples_map = Reshape((4, 4, z_dim))(z_samples_repeat)\n",
    "z_f_1 = Concatenate()([z_samples_map, feature_map])\n",
    "z_f_2 = Concatenate()([z_samples_map, feature_map_shuffle])\n",
    "\n",
    "\n",
    "# 全局判别器\n",
    "z_in = Input(shape=(z_dim*2,))\n",
    "z = z_in\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "GlobalDiscriminator = Model(z_in, z)\n",
    "\n",
    "z_z_1_scores = GlobalDiscriminator(z_z_1)\n",
    "z_z_2_scores = GlobalDiscriminator(z_z_2)\n",
    "global_info_loss = - K.mean(K.log(z_z_1_scores + 1e-6) + K.log(1 - z_z_2_scores + 1e-6))\n",
    "\n",
    "\n",
    "# 局部判别器\n",
    "z_in = Input(shape=(None, None, z_dim*2))\n",
    "z = z_in\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "LocalDiscriminator = Model(z_in, z)\n",
    "\n",
    "z_f_1_scores = LocalDiscriminator(z_f_1)\n",
    "z_f_2_scores = LocalDiscriminator(z_f_2)\n",
    "local_info_loss = - K.mean(K.log(z_f_1_scores + 1e-6) + K.log(1 - z_f_2_scores + 1e-6))\n",
    "\n",
    "# 用来训练的模型\n",
    "model_train = Model(x_in, [z_z_1_scores, z_z_2_scores, z_f_1_scores, z_f_2_scores])\n",
    "model_train.add_loss(alpha * global_info_loss + beta * local_info_loss + gamma * prior_kl_loss)\n",
    "model_train.compile(optimizer=Adam(1e-3))\n",
    "\n",
    "model_train.fit(x_train, epochs=50, batch_size=64)\n",
    "model_train.save_weights('total_model.cifar10.weights')\n",
    "\n",
    "\n",
    "# 输出编码器的特征\n",
    "zs = encoder.predict(x_train, verbose=True)\n",
    "zs.mean() # 查看均值（简单观察先验分布有没有达到效果）\n",
    "zs.std() # 查看方差（简单观察先验分布有没有达到效果）\n",
    "\n",
    "\n",
    "# 随机选一张图片，输出最相近的图片\n",
    "# 可以选用欧氏距离或者cos值\n",
    "def sample_knn(path):\n",
    "    n = 10\n",
    "    topn = 10\n",
    "    figure1 = np.zeros((img_dim*n, img_dim*topn, 3))\n",
    "    figure2 = np.zeros((img_dim*n, img_dim*topn, 3))\n",
    "    zs_ = zs / (zs**2).sum(1, keepdims=True)**0.5\n",
    "    for i in range(n):\n",
    "        one = np.random.choice(len(x_train))\n",
    "        idxs = ((zs**2).sum(1) + (zs[one]**2).sum() - 2 * np.dot(zs, zs[one])).argsort()[:topn]\n",
    "        for j,k in enumerate(idxs):\n",
    "            digit = x_train[k]\n",
    "            figure1[i*img_dim: (i+1)*img_dim,\n",
    "                   j*img_dim: (j+1)*img_dim] = digit\n",
    "        idxs = np.dot(zs_, zs_[one]).argsort()[-n:][::-1]\n",
    "        for j,k in enumerate(idxs):\n",
    "            digit = x_train[k]\n",
    "            figure2[i*img_dim: (i+1)*img_dim,\n",
    "                   j*img_dim: (j+1)*img_dim] = digit\n",
    "    figure1 = (figure1 + 1) / 2 * 255\n",
    "    figure1 = np.clip(figure1, 0, 255)\n",
    "    figure2 = (figure2 + 1) / 2 * 255\n",
    "    figure2 = np.clip(figure2, 0, 255)\n",
    "    imageio.imwrite(path+'_l2.png', figure1)\n",
    "    imageio.imwrite(path+'_cos.png', figure2)\n",
    "\n",
    "\n",
    "sample_knn('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
