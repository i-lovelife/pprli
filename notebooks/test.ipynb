{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-17-22:26:37.003398\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(str(datetime.now()).replace(' ', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, d_loss: 1.1619673, g_loss: 2.6499748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, d_loss: 0.7598796, g_loss: 2.3790529\n",
      "iter: 2, d_loss: 0.33013046, g_loss: 2.2660987\n",
      "iter: 3, d_loss: -0.15972137, g_loss: 2.2375674\n",
      "iter: 4, d_loss: -0.6943612, g_loss: 2.1725602\n",
      "iter: 5, d_loss: -1.046011, g_loss: 2.1967733\n",
      "iter: 6, d_loss: -1.69959, g_loss: 2.1659462\n",
      "iter: 7, d_loss: -1.8935342, g_loss: 2.187563\n",
      "iter: 8, d_loss: -2.5547745, g_loss: 2.1548007\n",
      "iter: 9, d_loss: -2.5946853, g_loss: 2.1364574\n",
      "iter: 10, d_loss: -3.197938, g_loss: 2.030978\n",
      "iter: 11, d_loss: -3.111682, g_loss: 2.0165043\n",
      "iter: 12, d_loss: -3.6269517, g_loss: 1.8648634\n",
      "iter: 13, d_loss: -3.5609584, g_loss: 1.9107238\n",
      "iter: 14, d_loss: -4.0567017, g_loss: 1.786227\n",
      "iter: 15, d_loss: -3.8665533, g_loss: 1.9188635\n",
      "iter: 16, d_loss: -4.339187, g_loss: 1.6688561\n",
      "iter: 17, d_loss: -4.00148, g_loss: 1.9733388\n",
      "iter: 18, d_loss: -4.560177, g_loss: 1.5295339\n",
      "iter: 19, d_loss: -4.0432878, g_loss: 2.01428\n",
      "iter: 20, d_loss: -4.797739, g_loss: 1.4840455\n",
      "iter: 21, d_loss: -4.517222, g_loss: 1.7450576\n",
      "iter: 22, d_loss: -4.9717464, g_loss: 1.5527492\n",
      "iter: 23, d_loss: -4.7422915, g_loss: 1.4557531\n",
      "iter: 24, d_loss: -5.1404724, g_loss: 1.5873168\n",
      "iter: 25, d_loss: -4.851943, g_loss: 1.265068\n",
      "iter: 26, d_loss: -5.2730856, g_loss: 1.4389002\n",
      "iter: 27, d_loss: -5.0463543, g_loss: 1.1649296\n",
      "iter: 28, d_loss: -5.472413, g_loss: 1.1388354\n",
      "iter: 29, d_loss: -5.392176, g_loss: 1.0901237\n",
      "iter: 30, d_loss: -5.720834, g_loss: 1.0113966\n",
      "iter: 31, d_loss: -5.5071654, g_loss: 1.030571\n",
      "iter: 32, d_loss: -5.875885, g_loss: 0.89094627\n",
      "iter: 33, d_loss: -5.583109, g_loss: 1.018378\n",
      "iter: 34, d_loss: -6.0530496, g_loss: 0.7742514\n",
      "iter: 35, d_loss: -5.770278, g_loss: 0.8993837\n",
      "iter: 36, d_loss: -6.3943267, g_loss: 0.726922\n",
      "iter: 37, d_loss: -6.1572533, g_loss: 0.71606535\n",
      "iter: 38, d_loss: -6.494495, g_loss: 0.67415833\n",
      "iter: 39, d_loss: -6.3402042, g_loss: 0.58156365\n",
      "iter: 40, d_loss: -6.6803527, g_loss: 0.5946209\n",
      "iter: 41, d_loss: -6.484232, g_loss: 0.51862794\n",
      "iter: 42, d_loss: -6.840395, g_loss: 0.4629436\n",
      "iter: 43, d_loss: -6.7137156, g_loss: 0.46184283\n",
      "iter: 44, d_loss: -7.0360565, g_loss: 0.41349417\n",
      "iter: 45, d_loss: -6.8032055, g_loss: 0.3940484\n",
      "iter: 46, d_loss: -7.161579, g_loss: 0.3380341\n",
      "iter: 47, d_loss: -6.906862, g_loss: 0.34821117\n",
      "iter: 48, d_loss: -7.267084, g_loss: 0.3446911\n",
      "iter: 49, d_loss: -6.837803, g_loss: 0.28357226\n",
      "iter: 50, d_loss: -7.3063793, g_loss: 0.37739468\n",
      "iter: 51, d_loss: -6.816358, g_loss: 0.2605862\n",
      "iter: 52, d_loss: -7.4252877, g_loss: 0.3353415\n",
      "iter: 53, d_loss: -7.121432, g_loss: 0.22335756\n",
      "iter: 54, d_loss: -7.5715876, g_loss: 0.277412\n",
      "iter: 55, d_loss: -7.3436775, g_loss: 0.19032615\n",
      "iter: 56, d_loss: -7.7560015, g_loss: 0.15542674\n",
      "iter: 57, d_loss: -7.511015, g_loss: 0.13515085\n",
      "iter: 58, d_loss: -7.849623, g_loss: 0.12560618\n",
      "iter: 59, d_loss: -7.669677, g_loss: 0.10699731\n",
      "iter: 60, d_loss: -7.922924, g_loss: 0.054781616\n",
      "iter: 61, d_loss: -7.7192135, g_loss: 0.083425105\n",
      "iter: 62, d_loss: -8.110102, g_loss: 0.02209425\n",
      "iter: 63, d_loss: -7.7767997, g_loss: 0.044546366\n",
      "iter: 64, d_loss: -8.187234, g_loss: -0.00768286\n",
      "iter: 65, d_loss: -7.797319, g_loss: 0.10007393\n",
      "iter: 66, d_loss: -8.171238, g_loss: -0.01999265\n",
      "iter: 67, d_loss: -7.611518, g_loss: 0.2050696\n",
      "iter: 68, d_loss: -8.233119, g_loss: 0.06724912\n",
      "iter: 69, d_loss: -7.852743, g_loss: 0.100899816\n",
      "iter: 70, d_loss: -8.364652, g_loss: 0.060244977\n",
      "iter: 71, d_loss: -8.103887, g_loss: 0.0027083755\n",
      "iter: 72, d_loss: -8.485457, g_loss: 0.04580915\n",
      "iter: 73, d_loss: -8.149073, g_loss: -0.02034372\n",
      "iter: 74, d_loss: -8.575738, g_loss: -0.03423226\n",
      "iter: 75, d_loss: -8.387039, g_loss: -0.051463842\n",
      "iter: 76, d_loss: -8.656626, g_loss: -0.07512492\n",
      "iter: 77, d_loss: -8.404707, g_loss: -0.059241712\n",
      "iter: 78, d_loss: -8.746031, g_loss: -0.08542502\n",
      "iter: 79, d_loss: -8.484787, g_loss: -0.06843251\n",
      "iter: 80, d_loss: -8.837784, g_loss: -0.11160821\n",
      "iter: 81, d_loss: -8.301863, g_loss: -0.058092654\n",
      "iter: 82, d_loss: -8.74621, g_loss: -0.0553388\n",
      "iter: 83, d_loss: -8.082677, g_loss: 0.06776863\n",
      "iter: 84, d_loss: -8.809571, g_loss: -0.01609534\n",
      "iter: 85, d_loss: -8.623612, g_loss: -0.026566505\n",
      "iter: 86, d_loss: -8.9790745, g_loss: -0.04419917\n",
      "iter: 87, d_loss: -8.621025, g_loss: -0.08910251\n",
      "iter: 88, d_loss: -9.039663, g_loss: -0.090325475\n",
      "iter: 89, d_loss: -8.779884, g_loss: -0.08593339\n",
      "iter: 90, d_loss: -9.096493, g_loss: -0.11707586\n",
      "iter: 91, d_loss: -8.858115, g_loss: -0.112562\n",
      "iter: 92, d_loss: -9.243435, g_loss: -0.15567386\n",
      "iter: 93, d_loss: -8.915199, g_loss: -0.122528195\n",
      "iter: 94, d_loss: -9.217952, g_loss: -0.18640566\n",
      "iter: 95, d_loss: -9.022235, g_loss: -0.11160898\n",
      "iter: 96, d_loss: -9.291887, g_loss: -0.16436535\n",
      "iter: 97, d_loss: -8.925167, g_loss: -0.057842255\n",
      "iter: 98, d_loss: -9.301097, g_loss: -0.17207569\n",
      "iter: 99, d_loss: -8.732797, g_loss: 0.10002488\n",
      "iter: 100, d_loss: -9.258802, g_loss: -0.11468679\n",
      "iter: 101, d_loss: -9.0141735, g_loss: -0.028033316\n",
      "iter: 102, d_loss: -9.49466, g_loss: -0.106030345\n",
      "iter: 103, d_loss: -9.196723, g_loss: -0.09695035\n",
      "iter: 104, d_loss: -9.521119, g_loss: -0.08370203\n",
      "iter: 105, d_loss: -9.161587, g_loss: -0.12338859\n",
      "iter: 106, d_loss: -9.482765, g_loss: -0.07502532\n",
      "iter: 107, d_loss: -8.907495, g_loss: -0.07001394\n",
      "iter: 108, d_loss: -9.442074, g_loss: -0.08447093\n",
      "iter: 109, d_loss: -8.928895, g_loss: 0.08345926\n",
      "iter: 110, d_loss: -9.572617, g_loss: -0.07310474\n",
      "iter: 111, d_loss: -9.268758, g_loss: -0.03796214\n",
      "iter: 112, d_loss: -9.653709, g_loss: -0.06650394\n",
      "iter: 113, d_loss: -9.326455, g_loss: -0.054750085\n",
      "iter: 114, d_loss: -9.736395, g_loss: -0.09543675\n",
      "iter: 115, d_loss: -9.471903, g_loss: -0.10476929\n",
      "iter: 116, d_loss: -9.7786045, g_loss: -0.088144004\n",
      "iter: 117, d_loss: -9.487555, g_loss: -0.07534444\n",
      "iter: 118, d_loss: -9.829725, g_loss: -0.11992866\n",
      "iter: 119, d_loss: -9.506784, g_loss: -0.10520989\n",
      "iter: 120, d_loss: -9.826868, g_loss: -0.16238129\n",
      "iter: 121, d_loss: -9.603255, g_loss: -0.08041906\n",
      "iter: 122, d_loss: -9.897693, g_loss: -0.14153314\n",
      "iter: 123, d_loss: -9.536233, g_loss: -0.068808556\n",
      "iter: 124, d_loss: -9.953697, g_loss: -0.16856873\n",
      "iter: 125, d_loss: -9.491416, g_loss: -0.04518354\n",
      "iter: 126, d_loss: -9.954912, g_loss: -0.14342266\n",
      "iter: 127, d_loss: -9.533934, g_loss: 0.03335923\n",
      "iter: 128, d_loss: -9.860304, g_loss: -0.0936293\n",
      "iter: 129, d_loss: -9.189265, g_loss: 0.092885256\n",
      "iter: 130, d_loss: -9.72029, g_loss: 0.03115195\n",
      "iter: 131, d_loss: -9.107838, g_loss: 0.1822744\n",
      "iter: 132, d_loss: -9.91123, g_loss: 0.035114825\n",
      "iter: 133, d_loss: -9.645078, g_loss: 0.06865722\n",
      "iter: 134, d_loss: -9.974968, g_loss: 0.03586644\n",
      "iter: 135, d_loss: -9.625095, g_loss: -0.018577516\n",
      "iter: 136, d_loss: -9.977667, g_loss: 0.013735235\n",
      "iter: 137, d_loss: -9.693481, g_loss: 0.027130544\n",
      "iter: 138, d_loss: -10.070526, g_loss: -0.052340508\n",
      "iter: 139, d_loss: -9.80923, g_loss: -0.032419324\n",
      "iter: 140, d_loss: -10.140549, g_loss: -0.06769347\n",
      "iter: 141, d_loss: -9.737783, g_loss: 0.036626816\n",
      "iter: 142, d_loss: -10.077623, g_loss: -0.025509953\n",
      "iter: 143, d_loss: -9.753939, g_loss: 0.058478713\n",
      "iter: 144, d_loss: -10.095667, g_loss: -0.03603351\n",
      "iter: 145, d_loss: -9.714899, g_loss: 0.070067406\n",
      "iter: 146, d_loss: -10.191561, g_loss: -0.021647692\n",
      "iter: 147, d_loss: -9.833219, g_loss: 0.024309337\n",
      "iter: 148, d_loss: -10.174128, g_loss: -0.024108648\n",
      "iter: 149, d_loss: -9.785424, g_loss: -0.0045583844\n",
      "iter: 150, d_loss: -10.182471, g_loss: -0.030240953\n",
      "iter: 151, d_loss: -9.8907175, g_loss: -0.00386554\n",
      "iter: 152, d_loss: -10.217306, g_loss: -0.043822408\n",
      "iter: 153, d_loss: -9.784899, g_loss: 0.007867813\n",
      "iter: 154, d_loss: -10.186453, g_loss: -0.030343592\n",
      "iter: 155, d_loss: -9.725305, g_loss: 0.0614205\n",
      "iter: 156, d_loss: -10.196807, g_loss: -0.011366308\n",
      "iter: 157, d_loss: -9.979206, g_loss: 0.016381621\n",
      "iter: 158, d_loss: -10.361309, g_loss: -0.041795075\n",
      "iter: 159, d_loss: -9.818119, g_loss: 0.09071416\n",
      "iter: 160, d_loss: -10.294689, g_loss: 0.005194783\n",
      "iter: 161, d_loss: -9.946803, g_loss: 0.095647275\n",
      "iter: 162, d_loss: -10.281364, g_loss: 0.0010861754\n",
      "iter: 163, d_loss: -9.94827, g_loss: 0.029527962\n",
      "iter: 164, d_loss: -10.307452, g_loss: -0.035984874\n",
      "iter: 165, d_loss: -10.0524435, g_loss: 0.058669925\n",
      "iter: 166, d_loss: -10.293394, g_loss: -0.029058993\n",
      "iter: 167, d_loss: -10.018267, g_loss: 0.021627247\n",
      "iter: 168, d_loss: -10.322952, g_loss: -0.024396837\n",
      "iter: 169, d_loss: -9.991259, g_loss: 0.028958023\n",
      "iter: 170, d_loss: -10.372087, g_loss: -0.020632029\n",
      "iter: 171, d_loss: -9.807658, g_loss: 0.07417524\n",
      "iter: 172, d_loss: -10.250986, g_loss: 0.06834388\n",
      "iter: 173, d_loss: -10.001643, g_loss: 0.03319925\n",
      "iter: 174, d_loss: -10.372303, g_loss: -0.0078107715\n",
      "iter: 175, d_loss: -10.062777, g_loss: 0.02543503\n",
      "iter: 176, d_loss: -10.324573, g_loss: -0.036177337\n",
      "iter: 177, d_loss: -10.233534, g_loss: -0.058442235\n",
      "iter: 178, d_loss: -10.319843, g_loss: -0.032188356\n",
      "iter: 179, d_loss: -10.216282, g_loss: -0.09077859\n",
      "iter: 180, d_loss: -10.355164, g_loss: -0.1045295\n",
      "iter: 181, d_loss: -10.141225, g_loss: -0.11528045\n",
      "iter: 182, d_loss: -10.371454, g_loss: -0.10138202\n",
      "iter: 183, d_loss: -10.227428, g_loss: -0.15997249\n",
      "iter: 184, d_loss: -10.305049, g_loss: -0.17978483\n",
      "iter: 185, d_loss: -10.236457, g_loss: -0.1638344\n",
      "iter: 186, d_loss: -10.352494, g_loss: -0.21702254\n",
      "iter: 187, d_loss: -10.159488, g_loss: -0.17680883\n",
      "iter: 188, d_loss: -10.268232, g_loss: -0.25081068\n",
      "iter: 189, d_loss: -10.179391, g_loss: -0.23585862\n",
      "iter: 190, d_loss: -10.307601, g_loss: -0.24894482\n",
      "iter: 191, d_loss: -10.072908, g_loss: -0.27478445\n",
      "iter: 192, d_loss: -10.21646, g_loss: -0.32087094\n",
      "iter: 193, d_loss: -10.067727, g_loss: -0.31204182\n",
      "iter: 194, d_loss: -10.236644, g_loss: -0.3283785\n",
      "iter: 195, d_loss: -9.950396, g_loss: -0.35617918\n",
      "iter: 196, d_loss: -10.232015, g_loss: -0.3605289\n",
      "iter: 197, d_loss: -9.977022, g_loss: -0.38993102\n",
      "iter: 198, d_loss: -10.2051735, g_loss: -0.39632237\n",
      "iter: 199, d_loss: -9.9278145, g_loss: -0.4098541\n",
      "iter: 200, d_loss: -10.117688, g_loss: -0.4636662\n",
      "iter: 201, d_loss: -9.886718, g_loss: -0.429043\n",
      "iter: 202, d_loss: -9.994377, g_loss: -0.4708163\n",
      "iter: 203, d_loss: -9.804621, g_loss: -0.4700004\n",
      "iter: 204, d_loss: -10.145079, g_loss: -0.4995367\n",
      "iter: 205, d_loss: -9.726272, g_loss: -0.43392766\n",
      "iter: 206, d_loss: -10.126567, g_loss: -0.5028337\n",
      "iter: 207, d_loss: -9.789908, g_loss: -0.50198954\n",
      "iter: 208, d_loss: -10.177185, g_loss: -0.4686337\n",
      "iter: 209, d_loss: -9.86183, g_loss: -0.5269142\n",
      "iter: 210, d_loss: -10.234989, g_loss: -0.53028196\n",
      "iter: 211, d_loss: -9.945393, g_loss: -0.5752574\n",
      "iter: 212, d_loss: -10.211123, g_loss: -0.5958876\n",
      "iter: 213, d_loss: -9.857944, g_loss: -0.56242335\n",
      "iter: 214, d_loss: -10.247896, g_loss: -0.5953572\n",
      "iter: 215, d_loss: -9.962658, g_loss: -0.58716005\n",
      "iter: 216, d_loss: -10.299467, g_loss: -0.62724096\n",
      "iter: 217, d_loss: -9.969936, g_loss: -0.59736496\n",
      "iter: 218, d_loss: -10.358359, g_loss: -0.61779696\n",
      "iter: 219, d_loss: -10.0461645, g_loss: -0.61355704\n",
      "iter: 220, d_loss: -10.288444, g_loss: -0.67162806\n",
      "iter: 221, d_loss: -10.046472, g_loss: -0.6222493\n",
      "iter: 222, d_loss: -10.430138, g_loss: -0.6779188\n",
      "iter: 223, d_loss: -10.030424, g_loss: -0.63576156\n",
      "iter: 224, d_loss: -10.465063, g_loss: -0.67014706\n",
      "iter: 225, d_loss: -10.127434, g_loss: -0.6632294\n",
      "iter: 226, d_loss: -10.537481, g_loss: -0.6779223\n",
      "iter: 227, d_loss: -10.183278, g_loss: -0.6917283\n",
      "iter: 228, d_loss: -10.561186, g_loss: -0.67636484\n",
      "iter: 229, d_loss: -10.301098, g_loss: -0.7018946\n",
      "iter: 230, d_loss: -10.620601, g_loss: -0.7102089\n",
      "iter: 231, d_loss: -10.29674, g_loss: -0.72662205\n",
      "iter: 232, d_loss: -10.63623, g_loss: -0.7403087\n",
      "iter: 233, d_loss: -10.363868, g_loss: -0.72866875\n",
      "iter: 234, d_loss: -10.706861, g_loss: -0.76661307\n",
      "iter: 235, d_loss: -10.39583, g_loss: -0.77227974\n",
      "iter: 236, d_loss: -10.724148, g_loss: -0.7818934\n",
      "iter: 237, d_loss: -10.521716, g_loss: -0.80951214\n",
      "iter: 238, d_loss: -10.774148, g_loss: -0.82629037\n",
      "iter: 239, d_loss: -10.520185, g_loss: -0.7934118\n",
      "iter: 240, d_loss: -10.775915, g_loss: -0.76133794\n",
      "iter: 241, d_loss: -10.584614, g_loss: -0.8027713\n",
      "iter: 242, d_loss: -10.835449, g_loss: -0.8309677\n",
      "iter: 243, d_loss: -10.585316, g_loss: -0.8368391\n",
      "iter: 244, d_loss: -10.801499, g_loss: -0.8611246\n",
      "iter: 245, d_loss: -10.564475, g_loss: -0.85728645\n",
      "iter: 246, d_loss: -10.925246, g_loss: -0.8475884\n",
      "iter: 247, d_loss: -10.608302, g_loss: -0.8452158\n",
      "iter: 248, d_loss: -10.910242, g_loss: -0.8738063\n",
      "iter: 249, d_loss: -10.701935, g_loss: -0.8739074\n",
      "iter: 250, d_loss: -10.915992, g_loss: -0.85953355\n",
      "iter: 251, d_loss: -10.677055, g_loss: -0.8673732\n",
      "iter: 252, d_loss: -10.991458, g_loss: -0.8600722\n",
      "iter: 253, d_loss: -10.780951, g_loss: -0.8663692\n",
      "iter: 254, d_loss: -10.977658, g_loss: -0.9012703\n",
      "iter: 255, d_loss: -10.787235, g_loss: -0.8305471\n",
      "iter: 256, d_loss: -11.027235, g_loss: -0.8968936\n",
      "iter: 257, d_loss: -10.91457, g_loss: -0.90678537\n",
      "iter: 258, d_loss: -11.032955, g_loss: -0.90917134\n",
      "iter: 259, d_loss: -10.848764, g_loss: -0.8864656\n",
      "iter: 260, d_loss: -11.085023, g_loss: -0.9278088\n",
      "iter: 261, d_loss: -10.9526005, g_loss: -0.9253492\n",
      "iter: 262, d_loss: -11.131515, g_loss: -0.9314224\n",
      "iter: 263, d_loss: -10.940498, g_loss: -0.9396341\n",
      "iter: 264, d_loss: -11.130948, g_loss: -0.9665501\n",
      "iter: 265, d_loss: -10.997515, g_loss: -0.9644833\n",
      "iter: 266, d_loss: -11.063083, g_loss: -0.96066093\n",
      "iter: 267, d_loss: -10.981167, g_loss: -0.94361144\n",
      "iter: 268, d_loss: -11.10398, g_loss: -0.9993374\n",
      "iter: 269, d_loss: -11.078558, g_loss: -0.9799947\n",
      "iter: 270, d_loss: -11.167813, g_loss: -0.9850721\n",
      "iter: 271, d_loss: -11.06613, g_loss: -1.0223544\n",
      "iter: 272, d_loss: -11.146083, g_loss: -0.993379\n",
      "iter: 273, d_loss: -11.074534, g_loss: -0.99932456\n",
      "iter: 274, d_loss: -11.186986, g_loss: -0.99443793\n",
      "iter: 275, d_loss: -11.117612, g_loss: -1.0044804\n",
      "iter: 276, d_loss: -11.1757, g_loss: -1.0065713\n",
      "iter: 277, d_loss: -11.055453, g_loss: -1.0041752\n",
      "iter: 278, d_loss: -11.166849, g_loss: -0.960795\n",
      "iter: 279, d_loss: -11.044439, g_loss: -0.9775349\n",
      "iter: 280, d_loss: -11.1751585, g_loss: -0.96901345\n",
      "iter: 281, d_loss: -11.110432, g_loss: -1.0054951\n",
      "iter: 282, d_loss: -11.175133, g_loss: -1.0076331\n",
      "iter: 283, d_loss: -11.211715, g_loss: -1.0275275\n",
      "iter: 284, d_loss: -11.204536, g_loss: -1.0557332\n",
      "iter: 285, d_loss: -11.232158, g_loss: -1.0524365\n",
      "iter: 286, d_loss: -11.2340145, g_loss: -1.0704498\n",
      "iter: 287, d_loss: -11.287819, g_loss: -1.0616143\n",
      "iter: 288, d_loss: -11.324175, g_loss: -1.0937154\n",
      "iter: 289, d_loss: -11.300614, g_loss: -1.0791439\n",
      "iter: 290, d_loss: -11.352585, g_loss: -1.0747428\n",
      "iter: 291, d_loss: -11.264883, g_loss: -1.0964949\n",
      "iter: 292, d_loss: -11.355416, g_loss: -1.0989919\n",
      "iter: 293, d_loss: -11.356419, g_loss: -1.0895492\n",
      "iter: 294, d_loss: -11.300113, g_loss: -1.1019515\n",
      "iter: 295, d_loss: -11.318165, g_loss: -1.0717295\n",
      "iter: 296, d_loss: -11.350605, g_loss: -1.0826615\n",
      "iter: 297, d_loss: -11.349146, g_loss: -1.1010865\n",
      "iter: 298, d_loss: -11.348281, g_loss: -1.0682412\n",
      "iter: 299, d_loss: -11.222405, g_loss: -1.0685794\n",
      "iter: 300, d_loss: -11.335821, g_loss: -1.0477889\n",
      "iter: 301, d_loss: -11.203625, g_loss: -1.0839502\n",
      "iter: 302, d_loss: -11.383137, g_loss: -1.0884436\n",
      "iter: 303, d_loss: -11.271024, g_loss: -1.0856874\n",
      "iter: 304, d_loss: -11.404366, g_loss: -1.1040412\n",
      "iter: 305, d_loss: -11.394366, g_loss: -1.1324532\n",
      "iter: 306, d_loss: -11.385408, g_loss: -1.157359\n",
      "iter: 307, d_loss: -11.4901705, g_loss: -1.1443394\n",
      "iter: 308, d_loss: -11.4460335, g_loss: -1.1716114\n",
      "iter: 309, d_loss: -11.501735, g_loss: -1.1606424\n",
      "iter: 310, d_loss: -11.498373, g_loss: -1.189963\n",
      "iter: 311, d_loss: -11.456931, g_loss: -1.190333\n",
      "iter: 312, d_loss: -11.488388, g_loss: -1.221757\n",
      "iter: 313, d_loss: -11.569376, g_loss: -1.2050158\n",
      "iter: 314, d_loss: -11.509236, g_loss: -1.224181\n",
      "iter: 315, d_loss: -11.557732, g_loss: -1.220496\n",
      "iter: 316, d_loss: -11.529503, g_loss: -1.2439854\n",
      "iter: 317, d_loss: -11.632019, g_loss: -1.223082\n",
      "iter: 318, d_loss: -11.641978, g_loss: -1.2319312\n",
      "iter: 319, d_loss: -11.590456, g_loss: -1.2421677\n",
      "iter: 320, d_loss: -11.578778, g_loss: -1.2431102\n",
      "iter: 321, d_loss: -11.637484, g_loss: -1.2606984\n",
      "iter: 322, d_loss: -11.645009, g_loss: -1.2211893\n",
      "iter: 323, d_loss: -11.579016, g_loss: -1.262294\n",
      "iter: 324, d_loss: -11.6108465, g_loss: -1.2614412\n",
      "iter: 325, d_loss: -11.606081, g_loss: -1.2374564\n",
      "iter: 326, d_loss: -11.597048, g_loss: -1.2154559\n",
      "iter: 327, d_loss: -11.508831, g_loss: -1.2359265\n",
      "iter: 328, d_loss: -11.566148, g_loss: -1.2207491\n",
      "iter: 329, d_loss: -11.337171, g_loss: -1.1835183\n",
      "iter: 330, d_loss: -11.52379, g_loss: -1.2041533\n",
      "iter: 331, d_loss: -11.4329815, g_loss: -1.1987585\n",
      "iter: 332, d_loss: -11.66259, g_loss: -1.2434869\n",
      "iter: 333, d_loss: -11.711383, g_loss: -1.2576295\n",
      "iter: 334, d_loss: -11.683592, g_loss: -1.2670943\n",
      "iter: 335, d_loss: -11.739831, g_loss: -1.2965194\n",
      "iter: 336, d_loss: -11.685801, g_loss: -1.2606755\n",
      "iter: 337, d_loss: -11.725116, g_loss: -1.2863898\n",
      "iter: 338, d_loss: -11.679925, g_loss: -1.2798728\n",
      "iter: 339, d_loss: -11.720608, g_loss: -1.3045509\n",
      "iter: 340, d_loss: -11.697032, g_loss: -1.2979568\n",
      "iter: 341, d_loss: -11.768819, g_loss: -1.3225721\n",
      "iter: 342, d_loss: -11.760828, g_loss: -1.3275497\n",
      "iter: 343, d_loss: -11.852038, g_loss: -1.3119951\n",
      "iter: 344, d_loss: -11.785146, g_loss: -1.3177173\n",
      "iter: 345, d_loss: -11.822335, g_loss: -1.3121235\n",
      "iter: 346, d_loss: -11.787395, g_loss: -1.3215308\n",
      "iter: 347, d_loss: -11.826826, g_loss: -1.3277224\n",
      "iter: 348, d_loss: -11.78698, g_loss: -1.3273749\n",
      "iter: 349, d_loss: -11.810184, g_loss: -1.3537155\n",
      "iter: 350, d_loss: -11.797792, g_loss: -1.3199519\n",
      "iter: 351, d_loss: -11.844902, g_loss: -1.3260393\n",
      "iter: 352, d_loss: -11.849632, g_loss: -1.3241366\n",
      "iter: 353, d_loss: -11.755298, g_loss: -1.2984629\n",
      "iter: 354, d_loss: -11.85031, g_loss: -1.3108072\n",
      "iter: 355, d_loss: -11.806578, g_loss: -1.331612\n",
      "iter: 356, d_loss: -11.841652, g_loss: -1.302982\n",
      "iter: 357, d_loss: -11.828064, g_loss: -1.3221678\n",
      "iter: 358, d_loss: -11.894923, g_loss: -1.3326373\n",
      "iter: 359, d_loss: -11.908327, g_loss: -1.3492113\n",
      "iter: 360, d_loss: -11.927122, g_loss: -1.3478184\n",
      "iter: 361, d_loss: -11.955105, g_loss: -1.3351666\n",
      "iter: 362, d_loss: -11.985076, g_loss: -1.3796802\n",
      "iter: 363, d_loss: -11.99134, g_loss: -1.3526291\n",
      "iter: 364, d_loss: -11.971958, g_loss: -1.366487\n",
      "iter: 365, d_loss: -12.069958, g_loss: -1.3714924\n",
      "iter: 366, d_loss: -11.956009, g_loss: -1.3885802\n",
      "iter: 367, d_loss: -11.986335, g_loss: -1.3677964\n",
      "iter: 368, d_loss: -12.019294, g_loss: -1.3809876\n",
      "iter: 369, d_loss: -11.975581, g_loss: -1.3711566\n",
      "iter: 370, d_loss: -11.9885845, g_loss: -1.370077\n",
      "iter: 371, d_loss: -11.991975, g_loss: -1.3786329\n",
      "iter: 372, d_loss: -11.988288, g_loss: -1.3428489\n",
      "iter: 373, d_loss: -11.912592, g_loss: -1.3694243\n",
      "iter: 374, d_loss: -11.977562, g_loss: -1.3150331\n",
      "iter: 375, d_loss: -11.838032, g_loss: -1.3463945\n",
      "iter: 376, d_loss: -12.008026, g_loss: -1.3221633\n",
      "iter: 377, d_loss: -11.904917, g_loss: -1.3450621\n",
      "iter: 378, d_loss: -11.977712, g_loss: -1.3328536\n",
      "iter: 379, d_loss: -12.016184, g_loss: -1.3677217\n",
      "iter: 380, d_loss: -12.022121, g_loss: -1.3653888\n",
      "iter: 381, d_loss: -12.08266, g_loss: -1.3849896\n",
      "iter: 382, d_loss: -12.029271, g_loss: -1.3817542\n",
      "iter: 383, d_loss: -12.107552, g_loss: -1.3749902\n",
      "iter: 384, d_loss: -12.064872, g_loss: -1.3649141\n",
      "iter: 385, d_loss: -12.089958, g_loss: -1.3915017\n",
      "iter: 386, d_loss: -12.118885, g_loss: -1.3909428\n",
      "iter: 387, d_loss: -12.14309, g_loss: -1.399744\n",
      "iter: 388, d_loss: -12.118946, g_loss: -1.3938936\n",
      "iter: 389, d_loss: -12.127123, g_loss: -1.4016774\n",
      "iter: 390, d_loss: -12.129513, g_loss: -1.3854173\n",
      "iter: 391, d_loss: -12.161029, g_loss: -1.3750925\n",
      "iter: 392, d_loss: -12.17809, g_loss: -1.3813062\n",
      "iter: 393, d_loss: -12.173203, g_loss: -1.3783001\n",
      "iter: 394, d_loss: -12.209091, g_loss: -1.378061\n",
      "iter: 395, d_loss: -12.186408, g_loss: -1.4034005\n",
      "iter: 396, d_loss: -12.182008, g_loss: -1.4081048\n",
      "iter: 397, d_loss: -12.232175, g_loss: -1.3814645\n",
      "iter: 398, d_loss: -12.269554, g_loss: -1.4030368\n",
      "iter: 399, d_loss: -12.251729, g_loss: -1.3936623\n",
      "iter: 400, d_loss: -12.235844, g_loss: -1.3948822\n",
      "iter: 401, d_loss: -12.208026, g_loss: -1.3646435\n",
      "iter: 402, d_loss: -12.185367, g_loss: -1.3499719\n",
      "iter: 403, d_loss: -12.050669, g_loss: -1.3639923\n",
      "iter: 404, d_loss: -12.190679, g_loss: -1.3136482\n",
      "iter: 405, d_loss: -11.91721, g_loss: -1.3246579\n",
      "iter: 406, d_loss: -12.149807, g_loss: -1.3145075\n",
      "iter: 407, d_loss: -12.017674, g_loss: -1.2960541\n",
      "iter: 408, d_loss: -12.281477, g_loss: -1.3672621\n",
      "iter: 409, d_loss: -12.248694, g_loss: -1.3655181\n",
      "iter: 410, d_loss: -12.33069, g_loss: -1.3778645\n",
      "iter: 411, d_loss: -12.311424, g_loss: -1.3896543\n",
      "iter: 412, d_loss: -12.311446, g_loss: -1.3816154\n",
      "iter: 413, d_loss: -12.324369, g_loss: -1.3862935\n",
      "iter: 414, d_loss: -12.357548, g_loss: -1.3935868\n",
      "iter: 415, d_loss: -12.303454, g_loss: -1.4053992\n",
      "iter: 416, d_loss: -12.346387, g_loss: -1.3831736\n",
      "iter: 417, d_loss: -12.323465, g_loss: -1.3934773\n",
      "iter: 418, d_loss: -12.392252, g_loss: -1.3860557\n",
      "iter: 419, d_loss: -12.36994, g_loss: -1.4191368\n",
      "iter: 420, d_loss: -12.378215, g_loss: -1.403934\n",
      "iter: 421, d_loss: -12.381645, g_loss: -1.4062077\n",
      "iter: 422, d_loss: -12.396781, g_loss: -1.4033453\n",
      "iter: 423, d_loss: -12.369529, g_loss: -1.4071429\n",
      "iter: 424, d_loss: -12.413283, g_loss: -1.3912879\n",
      "iter: 425, d_loss: -12.385627, g_loss: -1.4174596\n",
      "iter: 426, d_loss: -12.392745, g_loss: -1.401353\n",
      "iter: 427, d_loss: -12.407907, g_loss: -1.4134024\n",
      "iter: 428, d_loss: -12.456261, g_loss: -1.3992763\n",
      "iter: 429, d_loss: -12.388679, g_loss: -1.3707523\n",
      "iter: 430, d_loss: -12.444029, g_loss: -1.3590568\n",
      "iter: 431, d_loss: -12.443957, g_loss: -1.3749703\n",
      "iter: 432, d_loss: -12.458756, g_loss: -1.3666737\n",
      "iter: 433, d_loss: -12.418002, g_loss: -1.3838154\n",
      "iter: 434, d_loss: -12.445884, g_loss: -1.3713144\n",
      "iter: 435, d_loss: -12.424556, g_loss: -1.3790423\n",
      "iter: 436, d_loss: -12.45577, g_loss: -1.3455608\n",
      "iter: 437, d_loss: -12.451098, g_loss: -1.3807267\n",
      "iter: 438, d_loss: -12.411978, g_loss: -1.3369075\n",
      "iter: 439, d_loss: -12.312168, g_loss: -1.3445108\n",
      "iter: 440, d_loss: -12.368521, g_loss: -1.2764683\n",
      "iter: 441, d_loss: -12.030522, g_loss: -1.275325\n",
      "iter: 442, d_loss: -12.348914, g_loss: -1.2482498\n",
      "iter: 443, d_loss: -12.076282, g_loss: -1.2483327\n",
      "iter: 444, d_loss: -12.404186, g_loss: -1.326249\n",
      "iter: 445, d_loss: -12.435316, g_loss: -1.3364902\n",
      "iter: 446, d_loss: -12.498464, g_loss: -1.3497696\n",
      "iter: 447, d_loss: -12.395826, g_loss: -1.3277601\n",
      "iter: 448, d_loss: -12.551516, g_loss: -1.3545976\n",
      "iter: 449, d_loss: -12.429839, g_loss: -1.3363137\n",
      "iter: 450, d_loss: -12.588968, g_loss: -1.3664435\n",
      "iter: 451, d_loss: -12.539476, g_loss: -1.3479112\n",
      "iter: 452, d_loss: -12.530931, g_loss: -1.3777556\n",
      "iter: 453, d_loss: -12.541266, g_loss: -1.3486447\n",
      "iter: 454, d_loss: -12.551276, g_loss: -1.3705161\n",
      "iter: 455, d_loss: -12.540557, g_loss: -1.3785199\n",
      "iter: 456, d_loss: -12.586698, g_loss: -1.3695784\n",
      "iter: 457, d_loss: -12.605238, g_loss: -1.3834229\n",
      "iter: 458, d_loss: -12.592317, g_loss: -1.3534318\n",
      "iter: 459, d_loss: -12.604834, g_loss: -1.3735374\n",
      "iter: 460, d_loss: -12.5907345, g_loss: -1.3810184\n",
      "iter: 461, d_loss: -12.630454, g_loss: -1.3648384\n",
      "iter: 462, d_loss: -12.60336, g_loss: -1.3461705\n",
      "iter: 463, d_loss: -12.608779, g_loss: -1.3381834\n",
      "iter: 464, d_loss: -12.619713, g_loss: -1.3575351\n",
      "iter: 465, d_loss: -12.605371, g_loss: -1.3343709\n",
      "iter: 466, d_loss: -12.620558, g_loss: -1.3577156\n",
      "iter: 467, d_loss: -12.593119, g_loss: -1.3116922\n",
      "iter: 468, d_loss: -12.67281, g_loss: -1.3676591\n",
      "iter: 469, d_loss: -12.602413, g_loss: -1.3228093\n",
      "iter: 470, d_loss: -12.637444, g_loss: -1.3518035\n",
      "iter: 471, d_loss: -12.649046, g_loss: -1.3105487\n",
      "iter: 472, d_loss: -12.622785, g_loss: -1.3273113\n",
      "iter: 473, d_loss: -12.596201, g_loss: -1.3160646\n",
      "iter: 474, d_loss: -12.60924, g_loss: -1.3133975\n",
      "iter: 475, d_loss: -12.513134, g_loss: -1.2992973\n",
      "iter: 476, d_loss: -12.629292, g_loss: -1.2747978\n",
      "iter: 477, d_loss: -12.337584, g_loss: -1.2220845\n",
      "iter: 478, d_loss: -12.59007, g_loss: -1.1919247\n",
      "iter: 479, d_loss: -12.2915745, g_loss: -1.1972302\n",
      "iter: 480, d_loss: -12.654462, g_loss: -1.2243851\n",
      "iter: 481, d_loss: -12.47859, g_loss: -1.2999411\n",
      "iter: 482, d_loss: -12.662496, g_loss: -1.312796\n",
      "iter: 483, d_loss: -12.664231, g_loss: -1.3137635\n",
      "iter: 484, d_loss: -12.6960945, g_loss: -1.2970978\n",
      "iter: 485, d_loss: -12.666801, g_loss: -1.3105038\n",
      "iter: 486, d_loss: -12.748771, g_loss: -1.3101592\n",
      "iter: 487, d_loss: -12.637492, g_loss: -1.3059505\n",
      "iter: 488, d_loss: -12.706147, g_loss: -1.3029689\n",
      "iter: 489, d_loss: -12.739065, g_loss: -1.3104215\n",
      "iter: 490, d_loss: -12.724619, g_loss: -1.3140507\n",
      "iter: 491, d_loss: -12.766546, g_loss: -1.3109142\n",
      "iter: 492, d_loss: -12.709257, g_loss: -1.3396378\n",
      "iter: 493, d_loss: -12.756135, g_loss: -1.3291086\n",
      "iter: 494, d_loss: -12.740723, g_loss: -1.3203721\n",
      "iter: 495, d_loss: -12.775838, g_loss: -1.3166839\n",
      "iter: 496, d_loss: -12.762567, g_loss: -1.3226392\n",
      "iter: 497, d_loss: -12.731324, g_loss: -1.327148\n",
      "iter: 498, d_loss: -12.80791, g_loss: -1.312623\n",
      "iter: 499, d_loss: -12.717573, g_loss: -1.2937075\n",
      "iter: 500, d_loss: -12.754692, g_loss: -1.287864\n",
      "iter: 501, d_loss: -12.688677, g_loss: -1.3079886\n",
      "iter: 502, d_loss: -12.733464, g_loss: -1.2447331\n",
      "iter: 503, d_loss: -12.618464, g_loss: -1.250094\n",
      "iter: 504, d_loss: -12.725487, g_loss: -1.1948531\n",
      "iter: 505, d_loss: -12.48041, g_loss: -1.2058514\n",
      "iter: 506, d_loss: -12.706211, g_loss: -1.2221483\n",
      "iter: 507, d_loss: -12.466279, g_loss: -1.2039632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-541b51b227fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m#acc_p_on_x = vae.evaluate_p_on_x()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m#print(f'acc_y_on_x = {acc_y_on_x}, acc_p_on_x={acc_p_on_x}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-541b51b227fc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(gpu, total_iter, debug)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mae_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAeGan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m#vae.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mae_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;31m#acc_y_on_x = vae.evaluate_y_on_x()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m#acc_p_on_x = vae.evaluate_p_on_x()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-541b51b227fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sample_dir, model_path, total_iter, batch_size, iters_per_sample)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mx_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_fake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mp_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/nanlh/work/pprli/pprli_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/nanlh/work/pprli/pprli_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/nanlh/work/pprli/pprli_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/nanlh/work/pprli/pprli_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Lambda, BatchNormalization, Conv2D, Reshape, Dense,\\\n",
    "                         Dropout, Activation, Flatten, LeakyReLU, Add, MaxPooling2D,\\\n",
    "                         GlobalMaxPooling2D, Subtract, Concatenate, Average, Conv2DTranspose,\\\n",
    "                         GlobalAveragePooling2D\n",
    "from keras.losses import categorical_crossentropy, mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.data.dataset import load_ferg\n",
    "from src.evaluation.resnet import resnet_v1\n",
    "from src import PROJECT_ROOT, EXPERIMENT_ROOT\n",
    "import imageio\n",
    "import os\n",
    "from src.util.callbacks import Evaluate\n",
    "from src.models.task import FergTask\n",
    "import click\n",
    "\n",
    "def empty_loss(y_true, y_pred):\n",
    "    return y_pred\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "def show_model(model):\n",
    "    print('-'*80)\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    print('-'*80)\n",
    "def evaluate_encoder(train_data, test_data, num_classes, batch_size=256, num_epochs=20):\n",
    "    decoder = build_classifier(num_classes)\n",
    "    x_train, y_train = train_data\n",
    "    x_test, y_test = test_data\n",
    "    decoder.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = decoder.fit(x=x_train, y=y_train, epochs=num_epochs,batch_size=batch_size,\\\n",
    "                validation_data=(x_test, y_test),verbose=0)\n",
    "    return np.max(history.history['val_acc'])\n",
    "\n",
    "def shuffling(x):\n",
    "    idxs = K.arange(0, K.shape(x)[0])\n",
    "    idxs = K.tf.random_shuffle(idxs)\n",
    "    return K.gather(x, idxs)\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "def kl_loss_func(args):\n",
    "    z_mean, z_log_var = args\n",
    "    loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return loss\n",
    "\n",
    "def spectral_norm(w, r=5):\n",
    "    w_shape = K.int_shape(w)\n",
    "    in_dim = np.prod(w_shape[:-1]).astype(int)\n",
    "    out_dim = w_shape[-1]\n",
    "    w = K.reshape(w, (in_dim, out_dim))\n",
    "    u = K.ones((1, in_dim))\n",
    "    for i in range(r):\n",
    "        v = K.l2_normalize(K.dot(u, w))\n",
    "        u = K.l2_normalize(K.dot(v, K.transpose(w)))\n",
    "    return K.sum(K.dot(K.dot(u, w), K.transpose(v)))\n",
    "\n",
    "\n",
    "def spectral_normalization(w):\n",
    "    return w / spectral_norm(w)\n",
    "\n",
    "\n",
    "class AeGan(FergTask):\n",
    "    def __init__(self, data_loader, z_dim=128, kl_loss_weight=0.001, debug=False):\n",
    "        self.z_dim = z_dim\n",
    "        self.kl_loss_weight = kl_loss_weight\n",
    "        super().__init__(data_loader, debug)\n",
    "    def build_model(self):\n",
    "        input_shape = self.input_shape\n",
    "        img_dim = self.img_dim\n",
    "        z_dim = self.z_dim\n",
    "        num_p = self.num_p\n",
    "        encoder = self.build_encoder(input_shape, z_dim)\n",
    "        decoder = self.build_decoder(z_dim, img_dim, num_p)\n",
    "        classifier = self.build_classifier(input_shape, num_p, z_dim)\n",
    "        generator = self.build_generator(input_shape, num_p, encoder, decoder, classifier)\n",
    "        discriminator = self.build_discriminator(input_shape, num_p, classifier)\n",
    "        model = [encoder, decoder, classifier, generator, discriminator]\n",
    "        return model\n",
    "    def summary(self):\n",
    "        for model in self.model:\n",
    "            model.summary()\n",
    "            print('-' * 80)\n",
    "    def build_encoder(self, input_shape, z_dim):\n",
    "        x_in = Input(input_shape)\n",
    "        x = x_in\n",
    "        field_size = 8\n",
    "        for i in range(3):\n",
    "            x = Conv2D(int(z_dim / 2**(2-i)),\n",
    "                       kernel_size=(field_size, field_size),\n",
    "                       padding='SAME')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "        x = Dense(z_dim)(x)\n",
    "        return Model(x_in, x)\n",
    "        #z_mean = Dense(z_dim)(x)\n",
    "        #z_log_var = Dense(z_dim)(x)\n",
    "        #sample = Lambda(sampling)([z_mean, z_log_var])\n",
    "        #return Model(x_in, [sample, z_mean, z_log_var])\n",
    "    def build_decoder(self, z_dim, img_dim, num_p):\n",
    "        k = 8\n",
    "        units = z_dim\n",
    "        x_in = Input(shape=(z_dim,))\n",
    "        p_in =Input(shape=(num_p,))\n",
    "        h = Concatenate()([x_in, p_in])\n",
    "        h = Dense(4 * 4 * 128, activation='relu')(h)\n",
    "        h = Reshape((4, 4, 128))(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        h = Conv2DTranspose(units, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 32*32*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        # h = UpSampling2D(size=(2, 2))(h)\n",
    "        h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 64*64*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        # h = UpSampling2D(size=(2, 2))(h)\n",
    "        h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 8*6*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "\n",
    "        h = Conv2DTranspose(3, (k, k), strides=(2, 2), padding='same', activation='tanh')(h)  # 8*6*64\n",
    "        return Model([x_in, p_in], h)\n",
    "    def build_classifier(self, input_shape, num_p, z_dim):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        x = x_in\n",
    "        field_size = 8\n",
    "        for i in range(3):\n",
    "            x = Conv2D(int(z_dim / 2**(2-i)),\n",
    "                       kernel_size=(field_size, field_size),\n",
    "                       kernel_constraint=spectral_normalization,\n",
    "                       padding='SAME')(x)\n",
    "            x = BatchNormalization(gamma_constraint=spectral_normalization)(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "        predict_p = Dense(num_p, activation='softmax')(x)\n",
    "        predict_q = Dense(1, use_bias=False, kernel_constraint=spectral_normalization)(x)\n",
    "        return Model(x_in, [predict_p, predict_q])\n",
    "    def build_generator(self, input_shape, num_p, encoder, decoder, classifier):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        p_in_real = Input(shape=(num_p,))\n",
    "        p_in_fake = Input(shape=(num_p,))\n",
    "        x = x_in\n",
    "        x = encoder(x)\n",
    "        x_hat_fake = decoder([x, p_in_fake])\n",
    "        x_hat_real = decoder([x, p_in_real])\n",
    "        def mse_loss_func(args):\n",
    "            y_true, y_pred = args\n",
    "            return K.mean(K.square(y_pred - y_true))\n",
    "        mse_loss = Lambda(mse_loss_func, name='mse')([x_hat_real, x_in])\n",
    "        fake_p, fake_q = classifier(x_hat_fake)\n",
    "        def p_loss_func(args):\n",
    "            y_true, y_pred = args\n",
    "            return K.mean(categorical_crossentropy(y_true, y_pred))\n",
    "        p_loss = Lambda(p_loss_func)([p_in_fake, fake_p])\n",
    "        def q_loss_func(args):\n",
    "            fake_q = args\n",
    "            return K.mean(- fake_q)\n",
    "        q_loss = Lambda(q_loss_func)(fake_q)\n",
    "        def add_func(args):\n",
    "            p_loss, q_loss, mse_loss = args\n",
    "            return p_loss + q_loss + mse_loss\n",
    "        loss = Lambda(add_func)([p_loss, q_loss, mse_loss])\n",
    "        model = Model([x_in, p_in_real, p_in_fake], loss)\n",
    "        classifier.trainable = False\n",
    "        model.compile(optimizer=Adam(1e-4), loss=empty_loss)\n",
    "        return model\n",
    "    def build_discriminator(self, input_shape, num_p, classifier):\n",
    "        x_fake = Input(shape=input_shape)\n",
    "        p_fake = Input(shape=(num_p,))\n",
    "        x_sample = Input(shape=input_shape)\n",
    "        p_sample = Input(shape=(num_p,))\n",
    "        \n",
    "        p_fake_pre, q_fake_pre = classifier(x_fake)\n",
    "        p_sample_pre, q_sample_pre = classifier(x_sample)\n",
    "        #p_loss\n",
    "        def p_loss_func(args):\n",
    "            y_true, y_pred = args\n",
    "            return K.mean(categorical_crossentropy(y_true, y_pred))\n",
    "        p_loss_layer = Lambda(p_loss_func)\n",
    "        p_loss_fake = p_loss_layer([p_fake, p_fake_pre])\n",
    "        p_loss_sample = p_loss_layer([p_sample, p_sample_pre])\n",
    "        def average_loss_func(args):\n",
    "            p_loss_fake, p_loss_sample = args\n",
    "            return (p_loss_fake+p_loss_sample)/2\n",
    "        p_loss = Lambda(average_loss_func)([p_loss_fake, p_loss_sample])\n",
    "        #q_loss\n",
    "        def q_loss_func(args):\n",
    "            q_sample_pre, q_fake_pre = args\n",
    "            return K.mean(q_fake_pre - q_sample_pre)\n",
    "        q_loss = Lambda(q_loss_func)([q_sample_pre, q_fake_pre])\n",
    "        def add_func(args):\n",
    "            p_loss, q_loss = args\n",
    "            return p_loss+q_loss\n",
    "        loss = Lambda(add_func)([p_loss, q_loss])\n",
    "        model = Model([x_fake, p_fake, x_sample, p_sample], loss)\n",
    "        classifier.trainable = True\n",
    "        model.compile(optimizer=Adam(1e-4), loss=empty_loss)\n",
    "        return model\n",
    "        \n",
    "    def predict(self, x, p_fake=None):\n",
    "        encoder, decoder, classifier, generator, discriminator = self.model\n",
    "        z = encoder.predict(x)\n",
    "        batch_size = x.shape[0]\n",
    "        if p_fake is None:\n",
    "            p_fake = np.random.randint(self.num_p, size=batch_size)\n",
    "            p_fake = to_categorical(p_fake, self.num_p)\n",
    "        x_hat_fake = decoder.predict([z, p_fake])\n",
    "        return x_hat_fake\n",
    "    def train(self, sample_dir=None, model_path=None, total_iter=200000, batch_size=128, iters_per_sample=1000):\n",
    "        #unfinished\n",
    "        sample_dir = EXPERIMENT_ROOT / type(self).__name__ / 'sample'\n",
    "        sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "        model_path = EXPERIMENT_ROOT / type(self).__name__ / 'best.h5'\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        z_dim = self.z_dim\n",
    "        encoder, decoder, classifier, generator, discriminator = self.model\n",
    "        x_train, y_train, p_train = self.train_data\n",
    "        for i in range(total_iter):\n",
    "            idx1 = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "            idx2 = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "            p_fake = np.random.randint(self.num_p, size=batch_size)\n",
    "            p_fake = to_categorical(p_fake, self.num_p)\n",
    "            x_fake = self.predict(x_train[idx1], p_fake=p_fake)\n",
    "            for j in range(5):\n",
    "                d_loss = discriminator.train_on_batch([x_fake, p_fake, x_train[idx2], p_train[idx2]], p_train[idx1])\n",
    "            for j in range(1):\n",
    "                p_fake = np.random.randint(self.num_p, size=batch_size)\n",
    "                p_fake= to_categorical(p_fake, self.num_p)\n",
    "                g_loss = generator.train_on_batch([x_train[idx1], p_train[idx1], p_fake], p_train[idx1])\n",
    "            if i % 1 == 0:\n",
    "                print ('iter: %s, d_loss: %s, g_loss: %s' % (i, d_loss, g_loss))\n",
    "            if i % iters_per_sample == 0:\n",
    "                self.sample_all(sample_dir / f'{i}.png')\n",
    "        g_train_model.save_weights('./g_train_model.weights')\n",
    "            \n",
    "def main(gpu='3', total_iter=200000, debug=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "    loader = load_ferg()\n",
    "    ae_gan = AeGan(loader, debug=debug)\n",
    "    #vae.summary()\n",
    "    ae_gan.train()\n",
    "    #acc_y_on_x = vae.evaluate_y_on_x()\n",
    "    #acc_p_on_x = vae.evaluate_p_on_x()\n",
    "    #print(f'acc_y_on_x = {acc_y_on_x}, acc_p_on_x={acc_p_on_x}')\n",
    "main(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        choice(a, size=None, replace=True, p=None)\\n\\n        Generates a random sample from a given 1-D array\\n\\n                .. versionadded:: 1.7.0\\n\\n        Parameters\\n        -----------\\n        a : 1-D array-like or int\\n            If an ndarray, a random sample is generated from its elements.\\n            If an int, the random sample is generated as if a were np.arange(a)\\n        size : int or tuple of ints, optional\\n            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\\n            ``m * n * k`` samples are drawn.  Default is None, in which case a\\n            single value is returned.\\n        replace : boolean, optional\\n            Whether the sample is with or without replacement\\n        p : 1-D array-like, optional\\n            The probabilities associated with each entry in a.\\n            If not given the sample assumes a uniform distribution over all\\n            entries in a.\\n\\n        Returns\\n        --------\\n        samples : single item or ndarray\\n            The generated random samples\\n\\n        Raises\\n        -------\\n        ValueError\\n            If a is an int and less than zero, if a or p are not 1-dimensional,\\n            if a is an array-like of size 0, if p is not a vector of\\n            probabilities, if a and p have different lengths, or if\\n            replace=False and the sample size is greater than the population\\n            size\\n\\n        See Also\\n        ---------\\n        randint, shuffle, permutation\\n\\n        Examples\\n        ---------\\n        Generate a uniform random sample from np.arange(5) of size 3:\\n\\n        >>> np.random.choice(5, 3)\\n        array([0, 3, 4])\\n        >>> #This is equivalent to np.random.randint(0,5,3)\\n\\n        Generate a non-uniform random sample from np.arange(5) of size 3:\\n\\n        >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\\n        array([3, 3, 0])\\n\\n        Generate a uniform random sample from np.arange(5) of size 3 without\\n        replacement:\\n\\n        >>> np.random.choice(5, 3, replace=False)\\n        array([3,1,0])\\n        >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\\n\\n        Generate a non-uniform random sample from np.arange(5) of size\\n        3 without replacement:\\n\\n        >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\\n        array([2, 3, 0])\\n\\n        Any of the above can be repeated with an arbitrary array-like\\n        instead of just integers. For instance:\\n\\n        >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\\n        >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\\n        array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'],\\n              dtype='|S11')\\n\\n        \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
