{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Lambda, BatchNormalization, Conv2D, Reshape, Dense,\\\n",
    "                         Dropout, Activation, Flatten, LeakyReLU, Add, MaxPooling2D,\\\n",
    "                         GlobalMaxPooling2D, Subtract, Concatenate, Average, Conv2DTranspose,\\\n",
    "                         GlobalAveragePooling2D\n",
    "from keras.losses import categorical_crossentropy, mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.data.dataset import load_ferg\n",
    "from src.evaluation.resnet import resnet_v1\n",
    "from src import PROJECT_ROOT\n",
    "import imageio\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7, 6, (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "loader = load_ferg()\n",
    "(x_train, y_train, p_train), (x_test, y_test, p_test) = loader.load_data()\n",
    "num_y, num_p = loader.get_num_classes()\n",
    "input_shape = x_train[0].shape\n",
    "print(f'{num_y}, {num_p}, {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_loss(y_true, y_pred):\n",
    "    return y_pred\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "def show_model(model):\n",
    "    print('-'*80)\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 47401 samples, validate on 8365 samples\n",
      "Epoch 1/20\n",
      "47401/47401 [==============================] - 66s 1ms/step - loss: 0.3611 - acc: 0.9660 - val_loss: 0.2340 - val_acc: 0.9970\n",
      "Epoch 2/20\n",
      "47401/47401 [==============================] - 47s 987us/step - loss: 0.2102 - acc: 0.9993 - val_loss: 0.9781 - val_acc: 0.7668\n",
      "Epoch 3/20\n",
      "47401/47401 [==============================] - 47s 988us/step - loss: 0.1850 - acc: 0.9978 - val_loss: 0.2655 - val_acc: 0.9629\n",
      "Epoch 4/20\n",
      "47401/47401 [==============================] - 47s 990us/step - loss: 0.1505 - acc: 0.9994 - val_loss: 0.1353 - val_acc: 0.9996\n",
      "Epoch 5/20\n",
      "47401/47401 [==============================] - 47s 993us/step - loss: 0.1231 - acc: 0.9994 - val_loss: 0.1248 - val_acc: 0.9957\n",
      "Epoch 6/20\n",
      "47401/47401 [==============================] - 47s 992us/step - loss: 0.1026 - acc: 0.9992 - val_loss: 0.1035 - val_acc: 0.9964\n",
      "Epoch 7/20\n",
      "47401/47401 [==============================] - 47s 994us/step - loss: 0.0862 - acc: 0.9989 - val_loss: 0.8840 - val_acc: 0.8779\n",
      "Epoch 8/20\n",
      "47401/47401 [==============================] - 47s 997us/step - loss: 0.0722 - acc: 0.9998 - val_loss: 0.0660 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "47401/47401 [==============================] - 47s 1ms/step - loss: 0.0658 - acc: 0.9988 - val_loss: 0.0611 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "47401/47401 [==============================] - 47s 1ms/step - loss: 0.0517 - acc: 0.9999 - val_loss: 0.0478 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "47401/47401 [==============================] - 47s 998us/step - loss: 0.0485 - acc: 0.9993 - val_loss: 0.0496 - val_acc: 0.9982\n",
      "Epoch 12/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0413 - acc: 0.9997 - val_loss: 0.0382 - val_acc: 0.9996\n",
      "Epoch 13/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0356 - acc: 0.9996 - val_loss: 0.0359 - val_acc: 0.9993\n",
      "Epoch 14/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0353 - acc: 0.9993 - val_loss: 0.0411 - val_acc: 0.9973\n",
      "Epoch 15/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0316 - acc: 0.9995 - val_loss: 0.0296 - val_acc: 0.9999\n",
      "Epoch 16/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0338 - acc: 0.9991 - val_loss: 0.0373 - val_acc: 0.9996\n",
      "Epoch 17/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0303 - acc: 0.9997 - val_loss: 0.0272 - val_acc: 0.9999\n",
      "Epoch 18/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0250 - acc: 0.9998 - val_loss: 0.0247 - val_acc: 0.9998\n",
      "Epoch 19/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0235 - acc: 0.9996 - val_loss: 0.0215 - val_acc: 0.9999\n",
      "Epoch 20/20\n",
      "47401/47401 [==============================] - 48s 1ms/step - loss: 0.0220 - acc: 0.9997 - val_loss: 0.0204 - val_acc: 0.9999\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "def evaluate_encoder(train_data, test_data, num_classes, batch_size=256, num_epochs=20):\n",
    "    decoder = build_classifier(num_classes)\n",
    "    x_train, y_train = train_data\n",
    "    x_test, y_test = test_data\n",
    "    decoder.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = decoder.fit(x=x_train, y=y_train, epochs=num_epochs,batch_size=batch_size,\\\n",
    "                validation_data=(x_test, y_test),verbose=0)\n",
    "    return np.max(history.history['val_acc'])\n",
    "\n",
    "def shuffling(x):\n",
    "    idxs = K.arange(0, K.shape(x)[0])\n",
    "    idxs = K.tf.random_shuffle(idxs)\n",
    "    return K.gather(x, idxs)\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "def kl_loss_func(args):\n",
    "    z_mean, z_log_var = args\n",
    "    loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return loss\n",
    "def rec_loss_func(args):\n",
    "    y_true, y_pred = args\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "def categorical_loss_func(args):\n",
    "    y_true, y_pred = args\n",
    "    return categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "def build_mi_1(z_dim):\n",
    "    z_in = Input(shape=(z_dim*2,))\n",
    "    z = z_in\n",
    "    z = Dense(z_dim, activation='relu')(z)\n",
    "    z = Dense(z_dim, activation='relu')(z)\n",
    "    z = Dense(z_dim, activation='relu')(z)\n",
    "    z = Dense(1, activation='sigmoid')(z)\n",
    "    model_t = Model(z_in, z)\n",
    "    x = Input(shape=(z_dim,))\n",
    "    x_shuffle = Lambda(shuffling)(x)\n",
    "    real_score = model_t(Concatenate()([x, x]))\n",
    "    fake_score = model_t(Concatenate()([x, x_shuffle]))\n",
    "    def loss_func(args):\n",
    "        score1, score2 = args\n",
    "        return - K.mean(K.log(score1 + 1e-6) + K.log(1 - score2 + 1e-6))\n",
    "    loss = Lambda(loss_func)([real_score, fake_score])\n",
    "    model_mi = Model(x, loss)\n",
    "    return model_mi\n",
    "    \n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, task, save_path=None):\n",
    "        import os\n",
    "        self._task = task\n",
    "        self.lowest = 1e10\n",
    "        self.losses = []\n",
    "        if not os.path.exists('samples'):\n",
    "            os.mkdir('samples')\n",
    "        self.save_path = save_path\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        path = 'samples/test_%s.png' % epoch\n",
    "        self._task.sample_all(path)\n",
    "        self.losses.append((epoch, logs['loss']))\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            if self.save_path is not None:\n",
    "                self.model.save_weights(self.save_path)\n",
    "class Vae:\n",
    "    def __init__(self, data_loader, z_dim=128, debug=False):\n",
    "        #get data\n",
    "        if debug:\n",
    "            train_data, test_data = data_loader.load_data(max_train=100, max_test=100)\n",
    "        else:\n",
    "            train_data, test_data = data_loader.load_data()\n",
    "        num_y, num_p = data_loader.get_num_classes()\n",
    "        num_train = train_data[0].shape[0]\n",
    "        num_test = test_data[0].shape[0]\n",
    "        img_dim = test_data[0].shape[1]\n",
    "        input_shape = (img_dim, img_dim, 3)\n",
    "        train_data = Vae.transform_data(train_data, num_y, num_p)\n",
    "        test_data = Vae.transform_data(test_data, num_y, num_p)\n",
    "        #build model\n",
    "        encoder = self.build_encoder(input_shape, z_dim=z_dim)\n",
    "        decoder = self.build_decoder(z_dim, img_dim)\n",
    "        classifier = self.build_classifier(input_shape, num_y)\n",
    "        cvae = self.build_cvae(input_shape, num_y, encoder, decoder, classifier)\n",
    "        #assign variable\n",
    "        self.z_dim = z_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.input_shape = input_shape\n",
    "        self.num_y = num_y\n",
    "        self.num_p = num_p\n",
    "        self.num_train = num_train\n",
    "        self.num_test = num_test\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.cvae = cvae\n",
    "    def build_encoder(self, input_shape, z_dim):\n",
    "        x_in = Input(input_shape)\n",
    "        x = x_in\n",
    "        field_size = 8\n",
    "        for i in range(3):\n",
    "            x = Conv2D(int(z_dim / 2**(2-i)),\n",
    "                       kernel_size=(field_size, field_size),\n",
    "                       padding='SAME')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "        z_mean = Dense(z_dim)(x)\n",
    "        z_log_var = Dense(z_dim)(x)\n",
    "        x = Lambda(sampling)([z_mean, z_log_var])\n",
    "        return Model(x_in, [x, z_mean, z_log_var])\n",
    "    def build_decoder(self, z_dim, img_dim):\n",
    "        k = 8\n",
    "        units = z_dim\n",
    "        x = Input(shape=(z_dim,))\n",
    "        h = x\n",
    "        h = Dense(4 * 4 * 128, activation='relu')(h)\n",
    "        h = Reshape((4, 4, 128))(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        h = Conv2DTranspose(units, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 32*32*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        # h = UpSampling2D(size=(2, 2))(h)\n",
    "        h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 64*64*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "        # h = LeakyReLU(0.2)(h)\n",
    "        # h = UpSampling2D(size=(2, 2))(h)\n",
    "        h = Conv2DTranspose(units // 2, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 8*6*64\n",
    "        # h = Dropout(dropout)(h)\n",
    "        h = BatchNormalization(momentum=0.8)(h)\n",
    "\n",
    "        h = Conv2DTranspose(3, (k, k), strides=(2, 2), padding='same', activation='tanh')(h)  # 8*6*64\n",
    "        return Model(x, h, name=\"Decoder\")\n",
    "    def build_classifier(self, input_shape, num_classes):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        label_in = Input(shape=(num_classes,))\n",
    "        img_dim = input_shape[0]\n",
    "        x = x_in\n",
    "        x = Conv2D(img_dim // 2,\n",
    "                   (5, 5),\n",
    "                   strides=(2, 2),\n",
    "                   padding='same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        for i in range(4):\n",
    "            x = Conv2D(img_dim * 2**i,\n",
    "                       (5, 5),\n",
    "                       strides=(2, 2),\n",
    "                       padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU()(x)\n",
    "\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        out = Dense(num_classes, activation='softmax')(x)\n",
    "        loss = Lambda(categorical_loss_func)([label_in, out])\n",
    "        model = Model([x_in, label_in], loss)\n",
    "        return model\n",
    "    def build_cvae(self, input_shape, num_y, encoder, decoder,classifier):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        y_in = Input(shape=(num_y,))\n",
    "        z, z_mean, z_log_var = encoder(x_in)\n",
    "        kl_loss = Lambda(kl_loss_func)([z_mean, z_log_var])\n",
    "        x_rec = decoder(z)\n",
    "        classify_loss = classifier([x_rec, y_in])\n",
    "        rec_loss = Lambda(rec_loss_func)([x_in, x_rec])\n",
    "\n",
    "        def weight_loss_func(args):\n",
    "            kl_loss, rec_loss, classify_loss = args\n",
    "            return 0.005 * kl_loss + rec_loss + classify_loss\n",
    "        loss = Lambda(weight_loss_func)([kl_loss, rec_loss, classify_loss])\n",
    "        return Model([x_in, y_in], loss)\n",
    "    @staticmethod\n",
    "    def transform_data(data, num_y, num_p):\n",
    "        x, y, p= data\n",
    "        x = (x-127.5)/127.5\n",
    "        y = to_categorical(y, num_y)\n",
    "        p = to_categorical(p, num_p)\n",
    "        return (x, y, p)\n",
    "    @staticmethod\n",
    "    def recover_data(data):\n",
    "        x, y, p = data\n",
    "        x = (x + 1) / 2 * 255\n",
    "        x = x.astype(np.uint8)\n",
    "        x = np.clip(x, 0, 255)\n",
    "        y = np.argmax(y, axis=-1)\n",
    "        p = np.argmax(p, axis=-1)\n",
    "        return x, y, p\n",
    "    def sample_all(self, file_path):\n",
    "        x_train, y_train, p_train = self.train_data\n",
    "        num_y = self.num_y\n",
    "        num_p = self.num_p\n",
    "        num_data = self.num_train\n",
    "        img_dim = self.img_dim\n",
    "        output = np.zeros((2*num_p*img_dim, num_y*img_dim, 3))\n",
    "        for i in range(num_p):\n",
    "            for j in range(num_y):\n",
    "                for idx in range(num_data):\n",
    "                    if (np.argmax(p_train[idx]) == i) and (np.argmax(y_train[idx]) == j):\n",
    "                        x, y, p = x_train[idx], y_train[idx], p_train[idx]\n",
    "                        x_fake = self.predict_single(x)\n",
    "                        output[i*img_dim:(i+1)*img_dim, j*img_dim:(j+1)*img_dim,:] = Vae.recover_data((x, y, p))[0]\n",
    "                        output[i*img_dim+num_p*img_dim:(i+1)*img_dim+num_p*img_dim, \\\n",
    "                               j*img_dim:(j+1)*img_dim,:] = Vae.recover_data((x_fake, y, p))[0]\n",
    "                        break\n",
    "        imageio.imwrite(file_path, output)\n",
    "    def load_weights(self, path):\n",
    "        self.cvae.load_weights(path)\n",
    "    def predict(self, x):\n",
    "        z, z_mean, z_logvar = self.encoder.predict(x)\n",
    "        rec_x = self.decoder.predict(z)\n",
    "        return rec_x\n",
    "    def predict_single(self, x):\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        rec_x = self.predict(x)[0]\n",
    "        return rec_x\n",
    "    def evaluate_y(self, num_epochs=20, batch_size=128):\n",
    "        x_train, y_train, p_train = self.train_data\n",
    "        x_test, y_test, p_test = self.test_data\n",
    "        x_train = self.predict(x_train)\n",
    "        x_test = self.predict(x_test)\n",
    "        resnet = resnet_v1(self.input_shape, self.num_y)\n",
    "        history = resnet.fit(x_train, y_train, validation_data=(x_test, y_test), \\\n",
    "                             batch_size=batch_size, epochs=num_epochs)\n",
    "        acc = np.max(history.history['val_acc'])\n",
    "        print(acc)\n",
    "        return acc\n",
    "    def evaluate_p(self, num_epochs=20, batch_size=128):\n",
    "        x_train, y_train, p_train = self.train_data\n",
    "        x_test, y_test, p_test = self.test_data\n",
    "        x_train = self.predict(x_train)\n",
    "        x_test = self.predict(x_test)\n",
    "        resnet = resnet_v1(self.input_shape, self.num_p)\n",
    "        history = resnet.fit(x_train, p_train, validation_data=(x_test, p_test), \\\n",
    "                             batch_size=batch_size, epochs=num_epochs)\n",
    "        acc = np.max(history.history['val_acc'])\n",
    "        print(acc)\n",
    "        return acc\n",
    "    def train(self, save_path=None, num_epochs=20, batch_size=128):\n",
    "        self.cvae.compile(optimizer=Adam(1e-4), loss=empty_loss)\n",
    "        x_train, y_train, p_train = self.train_data\n",
    "        x_test, y_test, p_test = self.test_data\n",
    "        evaluator = Evaluate(self, save_path=save_path)\n",
    "        self.cvae.fit([x_train, y_train], y_train, validation_data=([x_test, y_test], y_test), \\\n",
    "                      batch_size=batch_size, epochs=num_epochs, callbacks=[evaluator])\n",
    "vae = Vae(loader, debug=False)\n",
    "vae.load_weights('./best.h5')\n",
    "#vae.evaluate_p()\n",
    "vae.evaluate_y()\n",
    "#vae.train(save_path='./best.h5', num_epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
